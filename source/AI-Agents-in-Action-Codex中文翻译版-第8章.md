# AI-Agents-in-Action-Codex中文翻译版-第8章

## 第8章 理解代理的记忆与知识

本章聚焦“检索增强生成（RAG）”如何为代理提供长期知识与记忆。我们先区分记忆/知识与提示工程策略的关系，再逐步实践：从 TF–IDF、余弦相似度到 OpenAI Embedding、Chroma DB、LangChain 文档加载/切分/检索，最后在 Nexus 中构建知识库与多种记忆库（对话、语义、情节、程序等），并讨论记忆/知识压缩的必要性。

---

## 8.1 记忆与检索在代理中的角色
- **知识**：外部文档（PDF、HTML、代码等）通过 RAG 提供上下文，帮助回答问题或引用资料。
- **记忆**：来自会话或交互日志的“事实与偏好”，可视为 RAG 机制在对话层面的应用。
- **提示工程映射**：
  - `Provide Reference Text` → 使用文档知识；
  - `Use External Tools` → 通过检索工具补充上下文；
  - `Memory` 策略 → 把历史对话或任务片段重新注入提示。

## 8.2 RAG 基础
1. **离线阶段**：加载文档 → 切分 chunk → 生成向量 → 存入向量库。
2. **在线阶段**：用户查询 → 同样嵌入为向量 → 语义相似度检索 → 将检索结果填入 Prompt → 调用 LLM 生成回答（图 8.2）。
3. **记忆版 RAG**：相同流程，只是数据源换成会话片段或提炼过的记忆（图 8.3）。

## 8.3 语义搜索与文档索引
### 8.3.1 TF–IDF + 余弦相似度
- 以“天空是蓝色而美丽”为例，计算 `TF=1/6`、`IDF=log10(8/4)`，得到 `TF–IDF≈0.05`。
- 使用 `TfidfVectorizer` + `cosine_similarity`（Listing 8.1–8.2）比较文档之间的向量距离，得分范围 -1~1（相似度）或 0~2（距离）（图 8.4、8.5）。
- 简易“向量数据库”示例（Listing 8.3）演示如何根据关键词/短语检索 TF–IDF 向量。

### 8.3.3 使用嵌入（Embeddings）
- OpenAI `text-embedding-ada-002` 生成 1536 维向量，后用 PCA 降至 3 维可视化（Listing 8.4、图 8.6）。
- 相较 TF–IDF，嵌入能够捕捉更丰富的语义关系。

### 8.3.4 引入 Chroma 向量库
- 示例（Listing 8.5）展示如何用 OpenAI Embedding + Chroma DB 存储文本，再通过余弦距离（越小越相关）查询相似片段。

## 8.4 使用 LangChain 构建 RAG
- LangChain 流程（图 8.7、8.8）：文档加载 → 切分 → 嵌入 → 存储 → 检索链。

### 8.4.1 文本切分
- `UnstructuredHTMLLoader` 读取 Mother Goose 童谣；
- `RecursiveCharacterTextSplitter` 以 100 字符 + 25 重叠切块；
- 嵌入后可语义查询（Listing 8.6）。

### 8.4.2 Token 级切分
- `CharacterTextSplitter.from_tiktoken_encoder(chunk_size=50, overlap=10)` 以 token 粒度切块；
- `Chroma.from_documents` + `OpenAIEmbeddings` 建立向量库；
- 查询“谁让女孩哭？”即可返回语义匹配的韵文，如 Georgy Porgy（Listing 8.7-8.8）。

## 8.5 在 Nexus 中搭建知识库
1. 安装/运行 Nexus（Listing 8.9）。
2. 在 “Knowledge Store Manager” 新建知识库，导入 `back_to_the_future.txt` 剧本（图 8.9）。
3. 等待切分/嵌入完成后，可在“Embeddings”界面查看 3D 分布并运行查询（图 8.10）。
4. 在 Chat 页面为代理启用 `time_travel` 知识库（图 8.11），便可围绕剧本提问。
5. “Configuration” 标签允许选择切分器、chunk size、overlap 等（图 8.12）。

## 8.6 构建代理记忆
### 8.6.1 基础记忆
- 代理记忆可映射到人类的感官记忆、短期记忆、长期记忆（图 8.13）。
- Nexus 记忆库流程（图 8.14）：
  1. 用户输入 → 通过记忆函数抽取事实/偏好 → 嵌入存储。
  2. 询问时再做语义检索，将“记得的事实”注入 Prompt。
- 操作步骤：
  - 在 Memory 页面创建 `my_memory`，选择 Agent Engine，添加若干个人偏好（图 8.15）。
  - 记忆函数（Listing 8.10）会将对话总结为 JSON 结构再入库。
  - 在 Chat 中启用该记忆库，测试代理是否能记住偏好（图 8.16）。

### 8.6.2 语义、情节、程序记忆
- 语义记忆（Semantic）、情节记忆（Episodic）、程序记忆（Procedural）需要额外的“语义增强”步骤：把输入转换成多条“针对该记忆类型的提问”，再分别检索（图 8.17）。
- Nexus 中可在 Memory 配置页选择 Memory Type = `SEMANTIC`，查看对应的记忆/增强/总结提示词（图 8.18）。
- 图 8.19 展示了同一批事实在“普通记忆”与“语义记忆”中的存储差异。

## 8.7 记忆与知识压缩
- 随时间积累，记忆与知识库会冗余；可通过 **k-means 聚类 + 总结** 来压缩（图 8.20）。
- Nexus 提供压缩工具（图 8.21）：
  - 3D 聚类可视化；
  - 指定 LLM（建议 GPT-4）对每个簇执行 summarization；
  - 适用于知识库（一次性）与记忆库（周期性）。
- 建议策略：
  - 大量冗余时压缩；
  - 视场景决定频率（记忆需定期压缩，知识库通常在导入时压缩即可）；
  - 可多重压缩形成不同粒度的“知识层级”；
  - 甚至可把压缩后的知识作为初始记忆灌入代理。

## 8.8 练习方向
1. **加载/切分新文档**：尝试不同的文本文档与切分参数，观察检索效果。
2. **语义搜索实验**：对比 TF–IDF、Word2Vec、BERT 等向量化方案的检索差异。
3. **定制 RAG 流程**：选定业务场景，用 LangChain 设计并测试专属 RAG pipeline。
4. **知识库 + 压缩实验**：构建知识库，尝试多种切分/压缩策略，观察检索性能变化。
5. **多记忆库构建**：动手创建对话、语义、情节、程序等不同记忆库，并测试压缩后的检索表现。

## 本章小结
- 记忆与知识都是对 RAG 的不同应用：前者关注个体交互，后者聚焦外部文档。
- TF–IDF/余弦相似度是理解语义检索的入门；嵌入 + 向量库（Chroma、OpenAI Embedding）提供更精确的语义匹配。
- LangChain 抽象了“加载→切分→嵌入→存储→检索”流程，便于快速搭建 RAG。
- Nexus 的知识库与记忆库实现了文档问答、偏好记忆、语义记忆等功能，并允许自定义切分、记忆函数、压缩策略。
- 记忆/知识压缩通过聚类+总结减少冗余，对提升检索效率和回答准确性至关重要。

掌握上述知识后，你即可为代理构建属于自己的“长期记忆”与“权威知识库”，并在后续章节中进一步叠加推理、评估与规划能力。
