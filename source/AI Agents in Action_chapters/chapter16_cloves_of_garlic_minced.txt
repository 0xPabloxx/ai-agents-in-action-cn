Chapter 16: cloves of garlic, minced
Starting Page: 68
================================================================================

Fresh basil leaves (for garnish)
Salt and pepper to taste
Estimated Costs:
Chicken strips: $6.00
Marinara sauce: $3.00
Mozzarella cheese: $2.00
Parmesan cheese: $1.50
Spaghetti: $1.00
Garlic: $0.30
Basil: $0.50
Total estimated cost: $14.30
Cost per serving: approximately $7.15
Nutritional Values per Serving:
Calories: 600 kcal
Protein: 35 g
Carbohydrates: 75 g
Fat: 18 g
Instructions: (not shown)
Generated by the Culinary
Companion GPT Assistant
Figure 3.4
The output results of the Culinary Companion GPT
First, ask the
LLM to set the
foundation.

45
3.2
Building a GPT that can do data science
SECOND PROMPT:
okay, can you now write all those steps into instructions
to be used for a GPT Agent (LLM agent) to replicate all of
the above steps
THIRD PROMPT:
What is a famous personality that can embody the agent
data scientist and be able to present data to users?
The result of that conversation provided for the assistant instructions shown in listing
3.4. In this case, the assistant was named Data Scout, but feel free to name your assis-
tant what appeals to you.
This GPT, named Data Scout, is designed to assist users by analyzing CSV
files and providing insights like Nate Silver, a famous statistician known
for his accessible and engaging approach to data. Data Scout combines
rigorous analysis with a clear and approachable communication style,
making complex data insights understandable. It is equipped to handle
statistical testing, predictive modeling, data visualization, and more,
offering suggestions for further exploration based on solid data-driven
evidence.
Data Scout requires the user to upload a csv file of data they want to
analyze. After the user uploads the file you will perform the following
tasks:
Data Acquisition
Ask the user to upload a csv file of data.
Instructions: Use the pandas library to read the data from the CSV
file. Ensure the data is correctly loaded by displaying the first few rows
using df.head().
2. Exploratory Data Analysis (EDA)
Data Cleaning
Task: Identify and handle missing values, correct data types.
Instructions: Check for missing values using df.isnull().sum(). For
categorical data, consider filling missing values with the mode, and for
numerical data, use the median or mean. Convert data types if necessary
using df.astype().
Visualization
Task: Create visualizations to explore the data.
Instructions: Use matplotlib and seaborn to create histograms, scatter
plots, and box plots. For example, use sns.histplot() for histograms and
sns.scatterplot() for scatter plots.
Descriptive Statistics
Task: Calculate basic statistical measures.
Instructions: Use df.describe() to get a summary of the statistics and
df.mean(), df.median() for specific calculations.
Listing 3.4
Data Scout instructions
Then, ask the LLM to
convert the previous
steps to a more
formal process.
Finally, ask the LLM to
provide a personality
that can represent the
process.

46
CHAPTER 3
Engaging GPT assistants
3. Hypothesis Testing
Task: Test a hypothesis formulated based on the dataset.
Instructions: Depending on the data type, perform statistical tests
like the t-test or chi-squared test using scipy.stats. For example, use
stats.ttest_ind() for the t-test between two groups.
4. Predictive Modeling
Feature Engineering
Task: Enhance the dataset with new features.
Instructions: Create new columns in the DataFrame based on existing
data to capture additional information or relationships. Use operations
like df['new_feature'] = df['feature1'] / df['feature2'].
Model Selection
Task: Choose and configure a machine learning model.
Instructions: Based on the task (classification or regression), select
a model from scikit-learn, like RandomForestClassifier() or
LinearRegression(). Configure the model parameters.
Training and Testing
Task: Split the data into training and testing sets, then train the
model.
Instructions: Use train_test_split from scikit-learn to divide the
data. Train the model using model.fit(X_train, y_train).
Model Evaluation
Task: Assess the model performance.
Instructions: Use metrics like mean squared error (MSE) or accuracy.
Calculate these using metrics.mean_squared_error(y_test, y_pred) or
metrics.accuracy_score(y_test, y_pred).
5. Insights and Conclusions
Task: Interpret and summarize the findings from the analysis and
modeling.
Instructions: Discuss the model coefficients or feature importances.
Draw conclusions about the hypothesis and the predictive analysis. Suggest
real-world implications or actions based on the results.
6. Presentation
Task: Prepare a report or presentation.
Instructions: Summarize the process and findings in a clear and
accessible format, using plots and bullet points. Ensure that the
presentation is understandable for non-technical stakeholders.
After generating the instructions, you can copy and paste them into the Configure
panel in figure 3.5. Be sure to give the assistant the Code Interpretation tool (skill) by
selecting the corresponding checkbox. You don’t need to upload files here; the assis-
tant will allow file uploads when the Code Interpretation checkbox is enabled.
Now, we can test the assistant by uploading a CSV file and asking questions about
it. The source code folder for this chapter contains a file called netflix_titles.csv;
the top few rows are summarized in listing 3.5. Of course, you can use any CSV file you
want, but this exercise will use the Netflix example. Note that this dataset was down-
loaded from Kaggle, but you can use any other CSV if you prefer.

47
3.2
Building a GPT that can do data science
show_id,type,title,director,cast,country,date_added,
release_year,rating,duration,listed_in,description
s1,Movie,Dick Johnson Is Dead,Kirsten Johnson,,
United States,"September 25, 2021",2020,PG-13,90 min,
Documentaries,"As her father nears the end of his life,
filmmaker Kirsten Johnson stages his death in inventive
and comical ways to help them both face the inevitable."
We could upload the file and ask the assistant to do its thing, but for this exercise,
we’ll be more specific. Listing 3.6 shows the prompt and uploading the file to engage
the assistant (including Netflix_titles.csv in the request). This example filters the
results to Canada, but you can, of course, use any country you want to view.
Analyze the attached CSV and filter the results to the
country Canada and output any significant discoveries
in trends etc.
If you encounter problems with the assistant parsing the file, refresh your browser win-
dow and try again. Depending on your data and filter, the assistant will now use the
Code Interpreter as a data scientist would to analyze and extract trends in the data.
Listing 3.5
netflix_titles.csv (top row of data)
Listing 3.6
Prompting the Data Scout
Conversation starters provide a quick
description and guide the user.
Be sure the Code Interpreter is selected.
Figure 3.5
Turning on the Code Interpreter tool/skill
Comma-separated
list of columns
An example row
of data from the
dataset
You can select a
different country to
filter the data on.

48
CHAPTER 3
Engaging GPT assistants
Figure 3.6 shows the output generated for the prompt in listing 3.5 using the net-
flix_titles.csv file for data. Your output may look quite different if you select a dif-
ferent country or request another analysis.
The data science plots the assistant is building are created by writing and executing
code with the Code Interpreter. You can try this with other CSV files or, if you want,
different forms of data to analyze. You could even continue iterating with the assistant
to update the plots visually or analyze other trends.
Code interpretation is a compelling skill that you’ll likely add to many of your
agents for everything from calculations to custom formatting. In the next section, we
look at how to extend the capabilities of a GPT through custom actions.
Top 10 Popular Genres in Canadian Content
Ratings Distribution
Distribution of Content Types
Trend of Releases over the Years
count
count
count
rating
Children and Family
movies
Comedies
Dramas
Kid’s TV
International Movies
Thrillers
Action and Adventure
Independent Movies
Documentaries
Horror Movies
type
release_year
Figure 3.6
The output generated by the assistant as it analyzed the CSV data

49
3.3
Customizing a GPT and adding custom actions
3.3
Customizing a GPT and adding custom actions
In our next exercise, we’ll demonstrate the use of custom actions, which can signifi-
cantly extend the reach of your assistant. Adding custom actions to an agent requires
several components, from understanding the OpenAPI specification endpoint to con-
necting to a service. Therefore, before we add custom actions, we’ll build another
GPT in the next section to assist us.
3.3.1
Creating an assistant to build an assistant
Given GPTs’ capabilities, it only makes sense that we use one to assist in building oth-
ers. In this section, we’ll build a GPT that can help us create a service we can connect
as a custom action to another GPT. And yes, we’ll even use an LLM to begin construct-
ing our helper GPT.
The following listing shows the prompt for creating the instructions for our helper
GPT. This prompt is intended to generate the instructions for the assistant.
I want to create a GPT assistant that can generate a FastAPI service that
will perform some action to be specified. As part of the FastAPI code
generation, I want the assistant to generate the OpenAPI specification for
the endpoint. Please outline a set of instructions for this agent.
Listing 3.8 shows the bulk of the instructions generated for the prompt. The output
was then modified and slightly updated with specific information and other details.
Copy and paste those instructions from the file (assistant_builder.txt) into your
GPT. Be sure to select the Code Interpreter capability also.
This GPT is designed to assist users in generating FastAPI services
tailored to specific actions, complete with the corresponding OpenAPI
specifications for the endpoints. The assistant will provide code snippets
and guidance on structuring and documenting API services using FastAPI,
ensuring that the generated services are ready for integration and
deployment.
1.   Define the Action and Endpoint: First, determine the specific action
the FastAPI service should perform. This could be anything from fetching
data, processing information, or interacting with other APIs or databases.
2.    Design the API Endpoint: Decide on the HTTP method (GET, POST, PUT,
DELETE, etc.) and the endpoint URI structure. Define the input parameters
(path, query, or body parameters) and the expected response structure.
3. Generate FastAPI Code:
Setup FastAPI: Import FastAPI and other necessary libraries.
Create API Function: Write a Python function that performs the
desired action. This function should accept the defined input parameters
and return the appropriate response.
Listing 3.7
Prompting the helper design (in GPT Builder or ChatGPT)
Listing 3.8
Custom action assistant instructions

50
CHAPTER 3
Engaging GPT assistants
4. Decorate the Function: Use FastAPI's decorators (e.g.,
@app.get("/endpoint")) to link the function with the specified endpoint
and HTTP method.
Define Input and Output Models: Use Pydantic models to define the
structure of the input and output data. This ensures validation and
serialization of the data.
5. Generate OpenAPI Specification:
FastAPI automatically generates the OpenAPI specification based on
the endpoint definitions and Pydantic models. Ensure that all function
parameters and models are well-documented using docstrings and field
descriptions.
Optionally, customize the OpenAPI specification by adding
metadata, tags, or additional responses directly in the FastAPI decorators.
6. Deployment:
Describe to the user how to prepare the FastAPI application for
deployment.
Instruct them on how to use ngrok to deploy the
service and host it on the user's local machine.
After preparing the assistant, ensure everything is set in the Configure panel (includ-
ing setting the Code Interpreter checkbox), and then refresh your browser window.
This will prepare the assistant for a new session. You can request the kind of service
you want to build from here.
Listing 3.9 shows the request to the Custom Action Assistant to create a daily task
endpoint. If you understand how APIs work, you can suggest other options, such as
POST. Of course, you can also ask the assistant to guide you and create your service.
I want to define a GET endpoint that replies with my list of daily tasks
After you enter the prompt, the assistant will generate the code and instructions for
creating and running the FastAPI endpoint. The following listing shows an example
of the code generated from the previous request.
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
app = FastAPI()
class Task(BaseModel):
id: int
description: str
completed: bool
tasks = [
Task(id=1, description="Buy groceries", completed=False),
Task(id=2, description="Read a book", completed=True),
Listing 3.9
Prompt requesting task endpoint service
Listing 3.10
daily_tasks_api.py (generated from assistant)
This uses ngrok as an
example to deploy the
service locally.
Use Pydantic to create
a type for the task.
This is a static list of
tasks to demonstrate.

51
3.3
Customizing a GPT and adding custom actions
Task(id=3, description="Complete FastAPI project", completed=False),
]
@app.get("/tasks", response_model=List[Task])
async def get_tasks():
"""
Retrieve a list of daily tasks.
"""
return tasks
Enter the code into Visual Studio Code (VS Code), and confirm that fastapi and
uvicorn are installed with pip. Then, run the API using the command shown in the
following listing, which runs the API in the chapter source file.
uvicorn daily_tasks_api:app –reload
Open a browser to http://127.0.0.1:8000/docs, the default location for the Swagger
endpoint, as shown in figure 3.7.
Listing 3.11
Running the API
The tasks endpoint
Change the name of the module/file
if you’re using something different.
Navigate your browser to
http://127.0.0. l :8000/docs.
Click this link to open and view
the OpenAPI speciﬁcation in JSON.
Figure 3.7
Navigating the Swagger docs and getting the openapi.json document

52
CHAPTER 3
Engaging GPT assistants
Clicking the /openapi.json link will display the OpenAPI specification for the end-
point, as shown in listing 3.12 (JSON converted to YAML). You’ll need to copy and
save this document for later use when setting up the custom action on the agent. The
endpoint produces JSON, but you can also use specifications written in YAML.
openapi: 3.1.0
info:
title: FastAPI
version: 0.1.0
paths:
/tasks:
get:
summary: Get Tasks
description: Retrieve a list of daily tasks.
operationId: get_tasks_tasks_get
responses:
'200':
description: Successful Response
content:
application/json:
schema:
type: array
items:
$ref: '#/components/schemas/Task'
title: Response Get Tasks Tasks Get
components:
schemas:
Task:
type: object
properties:
id:
type: integer
title: Id
description:
type: string
title: Description
completed:
type: boolean
title: Completed
required:
- id
- description
- completed
title: Task
Before connecting an assistant to the service, you must set up and use ngrok to open a
tunnel to your local machine running the service. Prompt the GPT to provide the
Listing 3.12
OpenAPI specification for the task API

53
3.3
Customizing a GPT and adding custom actions
instructions and help you set up ngrok, and run the application to open an endpoint
to port 8000 on your machine, as shown in listing 3.13. If you change the port or use a
different configuration, you must update it accordingly.
./ngrok authtoken <YOUR_AUTHTOKEN>
./ngrok http 8000
After you run ngrok, you’ll see an external URL that you can now use to access the ser-
vice on your machine. Copy this URL for later use when setting up the assistant. In the
next section, we’ll create the assistant that consumes this service as a custom action.
3.3.2
Connecting the custom action to an assistant
With the service up and running on your machine and accessible externally via the
ngrok tunnel, we can build the new assistant. This time, we’ll create a simple assistant
to help us organize our daily tasks, where the tasks will be accessible from our locally
running task service.
Open the GPT interface and the Configure panel, and copy and paste the instruc-
tions shown in listing 3.14 into the new assistant. Be sure to name the assistant and
enter a helpful description as well. Also, turn on the Code Interpreter capability to
allow the assistant to create the final plot, showing the tasks.
Task Organizer is designed to help the user prioritize their daily tasks
based on urgency and time availability, providing structured guidance on
how to categorize tasks by urgency and suggesting optimal time blocks for
completing these tasks. It adopts a persona inspired by Tim Ferriss, known
for his focus on productivity and efficiency. It uses clear, direct
language and avoids making assumptions about the user's free time.
When you are done organizing the tasks create a plot
showing when and how the tasks will be completed.
Click the Create New Action button at the bottom of the panel. Figure 3.8 shows the
interface for adding a custom action. You must copy and paste the OpenAPI specifica-
tion for your service into the window. Then, you must add a new section called servers
and populate that with your URL, as shown in the figure.



Listing 3.13
Running ngrok (following the instructions setup)
Listing 3.14
Task Organizer (task_organizer_assistant.txt)
Enter your auth token
obtained from ngrok.com.
Opens a tunnel on port 8000
to external internet traffic
This feature requires
the Code Interpreter
to be enabled.

54
CHAPTER 3
Engaging GPT assistants
After the specification is set, you can test it by clicking the Test button. This will run a
test, and you’ll see the results shown in the conversation window, as shown in figure 3.9.
After you’re satisfied, everything is set. Refresh your browser window to reset the
session, and enter something like the prompt shown in listing 3.15. This will prompt
the agent to call the service to get your daily tasks, summarize the output, and solve
your task organization dilemma.
how should I organize my tasks for today?
The assistant should produce a plot of the task schedule at the end. If it gets this
wrong or the formatting isn’t what you prefer, you can add instructions to specify the
format/style the assistant should output.
Listing 3.15
Task Organizer prompt
Add the servers section to the YAML, and
enter the ngrok URL for your service.
Copy and paste the YAML or JSON
speciﬁcation into the window.
Figure 3.8
Adding a new custom action

55
3.3
Customizing a GPT and adding custom actions
You can improve the service, but if you make any changes to the API, the specification
in the assistant custom actions will need to be updated. From here, though, you can
add custom action services run from your computer or hosted as a service.
NOTE
Be aware that unknown users can activate custom actions if you pub-
lish an assistant for public consumption, so don’t expose services that charge
you a service fee or access private information unless that is your intention.
Likewise, services opened through an ngrok tunnel will be exposed through
the assistant, which may be of concern. Please be careful when publishing
agents that consume custom actions.
Custom actions are a great way to add dynamic functionality to an assistant, whether
for personal or commercial use. File uploads are a better option for providing an assis-
tant with static knowledge. The next section will explore using file uploads to extend
an assistant’s knowledge.
Returned list of tasks
Testing the endpoint
Figure 3.9
Testing the API service endpoint is correctly configured as a custom action

56
CHAPTER 3
Engaging GPT assistants
3.4
Extending an assistant’s knowledge using file uploads
If you’ve engaged with LLMs, you likely have heard about the retrieval augmented
generation (RAG) pattern. Chapter 8 will explore RAG in detail for the application of
both knowledge and memory. Detailed knowledge of RAG isn’t required to use the
file upload capability, but if you need some foundation, check out that chapter.
The GPT Assistants platform provides a knowledge capability called file uploads,
which allows you to populate the GPT with a static knowledge base about anything in
various formats. As of writing, the GPT Assistants platform allows you to upload up to
