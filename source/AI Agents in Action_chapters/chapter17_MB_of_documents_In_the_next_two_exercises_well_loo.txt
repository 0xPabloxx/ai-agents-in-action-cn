Chapter 17: MB of documents. In the next two exercises, we’ll look at two different GPTs
Starting Page: 80
================================================================================

designed to assist users with consuming books.
3.4.1
Building the Calculus Made Easy GPT
Books and written knowledge will always be the backbone of our knowledge base. But
reading text is a full-time concerted effort many people don’t have time for. Audio-
books made consuming books again accessible; you could listen while multitasking,
but not all books transitioned well to audio.
Enter the world of AI and intelligent assistants. With GPTs, we can create an inter-
active experience between the reader and the book. No longer is the reader forced to
consume a book page by page but rather as a whole.
To demonstrate this concept, we’ll build a GPT based on a classic math text called
Calculus Made Easy, by Silvanus P. Thompson. The book is freely available through the
Gutenberg Press website. While it’s more than a hundred years old, it still provides a
solid material background.
NOTE
If you’re serious about learning calculus but this assistant is still too
advanced, check out a great book by Clifford A. Pickover called Calculus and
Pizza. It’s a great book for learning calculus or just to get an excellent
refresher. You could also try making your Calculus and Pizza assistant if you
have an eBook version. Unfortunately, copyright laws would prevent you from
publishing this GPT without permission.
Open ChatGPT, go to My GPTs, create a new GPT, click the Configure tab, and then
upload the file, as shown in figure 3.10. Upload the book from the chapter’s source
code folder: chapter _03/calculus_made_easy.pdf. This will add the book to the
GPT’s knowledge.
Scroll up and add the instructions shown in listing 3.16. The initial preamble text
was generated by conversing with the GPT Builder. After updating the preamble text,
a personality was added by asking ChatGPT for famous mathematicians. Then, finally,
rules were added to provide additional guidance to the GPT on what explicit out-
comes we want.




57
3.4
Extending an assistant’s knowledge using file uploads
This GPT is designed to be an expert teacher and mentor
of calculus based on the book 'Calculus Made Easy' by
Silvanus Thompson. A copy of the book is uploaded at
calculus_made_easy.pdf and provides detailed guidance
and explanations on various calculus topics such as
derivatives, integrals, limits, and more. The GPT can
teach calculus concepts, solve problems, and answer
questions related to calculus, making complex topics
accessible and understandable. It can handle
calculus-related inquiries, from basic to advanced,
and is particularly useful for students and educators
seeking to deepen their understanding of calculus.
Listing 3.16
Instructions for Calculus Made Easy GPT
Adding ﬁles is considered giving your
assistant additional knowledge.
Use the Upload ﬁles button to add
various sources of static
knowledge for the assistant.
Be sure to enable Code Interpreter
so the assistant can demonstrate
concepts.
Figure 3.10
Adding files to the assistant’s knowledge
The preamble was
initially generated
by the Builder and
then tweaked as
needed.

58
CHAPTER 3
Engaging GPT assistants
Answer as the famous mathematician Terence Tao.
Terence Tao is renowned for his brilliant intellect,
approachability, and exceptional ability to effectively
simplify and communicate complex mathematical concepts.
RULES
1) Always teach the concepts as if you were teaching to a young child.
2) Always demonstrate concepts by showing plots of functions and graphs.
3) Always ask if the user wants to try a sample problem on their own.
Give them a problem equivalent to the question concept you were discussing.
After updating the assistant, you can try it in the preview window or the book version
by searching for Calculus Made Easy in the GPT Store. Figure 3.11 shows a snipped
example of interaction with the GPT. The figure shows that the GPT can generate
plots to demonstrate concepts or ask questions.
This GPT demonstrates the ability of an assistant to use a book as a companion
teaching reference. Only a single book was uploaded in this exercise, but multiple
books or other documents could be uploaded. As this feature and the technology
mature, in the future, it may be conceivable that an entire course could be taught
using a GPT.
We’ll move away from technical and embrace fiction to demonstrate the use of
knowledge. In the next section, we’ll look at how knowledge of file uploads can be
used for search and reference.
3.4.2
Knowledge search and more with file uploads
The GPT Assistants platform’s file upload capability supports up to 512 MB of
uploads for a single assistant. This feature alone provides powerful capabilities for
document search and other applications in personal and small-to-medium business/
project sizes.
Imagine uploading a whole collection of files. You can now search, compare, con-
trast, organize, and collate all with one assistant. This feature alone within GPT Assis-
tants will disrupt how we search for and analyze documents. In chapter 6, we’ll
examine how direct access to the OpenAI Assistants API can increase the number of
documents.
For this next exercise, we’ll employ an assistant with knowledge of multiple books
or documents. This technique could be applied to any supported document, but this
assistant will consume classic texts about robots. We’ll name this assistant the Classic
Robot Reads GPT.
Start by creating a new GPT assistant in the ChatGPT interface. Then, upload the
instructions in listing 3.17, and name and describe the assistant. These instructions
were generated in part through the GPT Builder and then edited.


Be sure always to give
your assistants and
agents an appropriate
persona/personality.
Defining explicit conditions and rules can help
better guide the GPT to your desire.

59
3.4
Extending an assistant’s knowledge using file uploads
This GPT, Classic Robot Reads and uses the persona of
Isaac Asimov and will reply as the famous robot author.
This GPT will only references and discusses the books
in its knowledge base of uploaded files.
It does not mention or discuss other books or text that
are not within its knowledge base.
RULES
Refer to only text within your knowledge base
Listing 3.17
Classic Robot Reads instructions
The GPT can also generate plots to
demonstrate concepts, such as showing
the function and its derivative.
The conversation was started by asking
the GPT to teach the basics of calculus.
Function and Its Derivative
Figure 3.11
Output from asking the GPT to teach calculus
Remember always
to give your GPT a
persona/personality.
Make sure the
assistant only
references knowledge
within file uploads.

60
CHAPTER 3
Engaging GPT assistants
Always provide 3 examples of any query the use asks for
Always ask the user if they require anything further
After completing those steps, you can upload the files from the chapter’s source called
gutenberg_robot_books. Figure 3.12 demonstrates uploading multiple files at a time.
The maximum number of files you can upload at a time will vary according to the
sizes of the files.
You can start using it after uploading the documents, setting the instructions, and giv-
ing the assistant a name and an image. Search is the most basic application of a knowl-
edge assistant, and other use cases in the form of prompts are shown in table 3.1.
Add some extra
rules for style
choices.
Make the assistant more helpful by
also giving them nuance and style.
You can upload multiple
ﬁles (about 5) at a time.
Uploads become accessible to the
agent through knowledge patterns.
Figure 3.12
Uploading documents to the assistant’s knowledge

61
3.5
Publishing your GPT
These use cases are just a sample of the many things possible with an AI knowledge
assistant. While this feature may not be poised to disrupt enterprise search, it gives
smaller organizations and individuals more access to their documents. It allows the
creation of assistants as a form of knowledge that can be exposed publicly. In the next
section, we’ll look at how to make assistants consumable by all.
3.5
Publishing your GPT
Once you’re happy with your GPT, you can use it or share it with others by providing a
link. Consuming GPT assistants through ChatGPT currently requires a Plus subscrip-
tion. To publish your GPT for others, click the Share button, and select your sharing
option, as shown in figure 3.13.
Table 3.1
Use cases for a knowledge assistant
Use case
Example prompt
Results
Search
Search for this phrase in your knowledge:
“the robot servant.”
Returns the document and an excerpt
Compare
Identify the three most similar books that
share the same writing style.
Returns the three most similar docu-
ments
Contrast
Identify the three most different books.
Returns books in the collection that are
the most different
Ordering
What order should I read the books?
Returns an ordered progression of books
Classification
Which of these books is the most modern?
Classifies documents
Generation
Generate a fictional paragraph that mimics
your knowledge of the robot servant.
Generates new content based on its
knowledge base
Allows you to give the
link to other users
If you give a link to a GPT or make it
public, usage of that assistant is taken
from the user’s account and not yours.
Only for you
Publishes your GPT to the
store and makes it public
Figure 3.13
GPT
sharing options

62
CHAPTER 3
Engaging GPT assistants
Whether you share your GPT with friends and colleagues or publicly in the GPT
Store, the assistant’s usage is taken from the account using it, not the publisher. This
means if you have a particularly expensive GPT that generates a lot of images, for
example, it won’t affect your account while others use it.
3.5.1
Expensive GPT assistants
At the time of writing, OpenAI tracks the resource usage of your ChatGPT account,
including that used for GPTs. If you hit a resource usage limit and get blocked, your
ChatGPT account will also be blocked. Blockages typically only last a couple of hours,
but this can undoubtedly be more than a little annoying.
Therefore, we want to ensure that users using your GPT don’t exceed their resource
usage limits for regular use. Following is a list of features that increase resource usage
while using the GPT:
Creating images—Image generation is still a premium service, and successive image
generation can quickly get your user blocked. It’s generally recommended that
you inform your users of the potential risks and/or try to reduce how fre-
quently images are generated.
Code interpretation—This feature allows for file uploads and running of code for
data analysis. If you think your users will require constant use of the coding tool,
then inform them of the risk.
Vision, describing images—If you’re building an assistant that uses vision to describe
and extract information from the image, plan to use it sparingly.
File uploads—If your GPT uses a lot of files or allows you to upload several files,
this may cause blocks. As always, guide the user away from anything preventing
them from enjoying your GPT.
NOTE
Moore’s Law states that computers will double in power every two years
while costing half as much. LLMs are now doubling in power about every six
months from optimization and increasing GPU power. This, combined with
the cost being reduced by at least half in the same period, likely means cur-
rent resource limits on vision and image-generation models won’t be consid-
ered. However, services such as code interpretation and file uploads will likely
remain the same.
Making your assistant aware of resource usage can be as simple as adding the rule
shown in listing 3.18 to the assistant’s instructions. The instructions can be just a state-
ment relaying the warning to the user and making the assistant aware. You could even
ask the assistant to limit its usage of certain features.
RULE:
When generating images, ensure the user is aware that creating multiple
images quickly could temporarily block their account.
Listing 3.18
Resource usage rule example

63
3.5
Publishing your GPT
Guiding your assistant to be more resource conscious in the end makes your assistant
more usable. It also helps prevent angry users who unknowingly get blocked using
your assistant. This may be important if you plan on releasing your GPT, but before
that, let’s investigate the economics in the next section.
3.5.2
Understanding the economics of GPTs
Upon the release of GPT Assistants and the GPT Store, OpenAI announced the
potential for a future profit-sharing program for those who published GPTs. While
we’re still waiting to hear more about this program, many have speculated what this
may look like.
Some have suggested the store may return only 10% to 20% of profits to the build-
ers. This is far less than the percentage on other app platforms but requires much less
technical knowledge and fewer resources. The GPT Store is flooded with essentially free
assistants, provided you have a Plus subscription, but that may change in the future.
Regardless, there are also several reasons why you may want to build public GPTs:
Personal portfolio—Perhaps you want to demonstrate your knowledge of prompt
engineering or your ability to build the next wave of AI applications. Having a
few GPTs in the GPT Store can help demonstrate your knowledge and ability to
create useful AI applications.
Knowledge and experience—If you have in-depth knowledge of a subject or topic,
this can be a great way to package that as an assistant. These types of assistants
will vary in popularity based on your area of expertise.
Cross-marketing and commercial tie-in—This is becoming more common in the
Store and provides companies the ability to lead customers using an assistant.
As companies integrate more AI, this will certainly be more common.
Helpful assistant to your product/service—Not all companies or organizations can
sustain the cost of hosting chatbots. While consuming assistants is currently lim-
ited to ChatGPT subscribers, they will likely be more accessible in the future.
This may mean having GPTs for everything, perhaps like the internet’s early
days where every company rushed to build a web presence.
While the current form of the GPT Store is for ChatGPT subscribers, if the current
trend with OpenAI continues, we’ll likely see a fully public GPT Store. Public GPTs
have the potential to disrupt the way we search, investigate products and services, and
consume the internet. In the last section of this chapter, we’ll examine how to publish
a GPT and some important considerations.
3.5.3
Releasing the GPT
Okay, you’re happy with your GPT and how it operates, and you see real benefit from
giving it to others. Publishing GPTs for public (subscribers) consumption is easy, as
shown in figure 3.14. After selecting the GPT Store as the option and clicking Save,
you’ll now have the option to set the category and provide links back to you.

64
CHAPTER 3
Engaging GPT assistants
That is easy, so here are a few more things you’ll want to consider before publishing
your GPT:
GPT description—Create a good description, and you may even want to ask
ChatGPT to help you build a description that increases the search engine opti-
mization (SEO) of your GPT. GPTs are now showing up in Google searches, so
good search engine optimization can help increase exposure to your assistant.
A good description will also help users decide if they want to take the time to
use your assistant.
The logo—A nice, clean logo that identifies what your assistant does can undoubt-
edly help. Logo design for GPTs is effectively a free service, but taking the time
to iterate over a few images can help draw users to your assistant.
The category—By default, the category will already be selected, but make sure it
fits your assistant. If you feel it doesn’t, than change the category, and you may
even want to select Other and define your own.
Links—Be sure to set reference links for your social media and perhaps even a
GitHub repository that you use to track problems for the GPT. Adding links to
your GPT demonstrates to users that they can reach out to the builder if they
encounter problems or have questions.
Selecting this allows
you to view set links.
You can set links to your
social media and GitHub.
The area you want to
publish your GPT to
Figure 3.14
Selecting the options after clicking Save to publish to the GPT Store

65
3.6
Exercises
Further requirements may likely emerge as the GPT Store matures. The business
model remains to be established, and other learnings will likely follow. Whether you
decide to build GPTs for yourself or others, doing so can help improve your under-
standing of how to build agents and assistants. As we’ll see throughout the rest of this
book, GPT assistants are a useful foundation for your knowledge.
3.6
Exercises
Complete the following exercises to improve your knowledge of the material:
Exercise 1—Build Your First GPT Assistant
Objective—Create a simple GPT assistant using the ChatGPT interface.
Tasks:
– Sign up for a ChatGPT Plus subscription if you don’t already have one.
– Navigate to the GPT Assistants platform, and click the Create button.
– Follow the Builder chat interface to create a Culinary Companion assistant
that provides meal suggestions based on available ingredients.
– Manually configure the assistant to add custom rules for recipe generation,
such as including nutritional information and cost estimates.
Exercise 2—Data Analysis Assistant
Objective—Develop a GPT assistant that can analyze CSV files and provide
insights.
Tasks:
– Design a data science assistant that can load and analyze CSV files, similar to
the Data Scout example in the chapter.
– Enable the Code Interpretation tool, and upload a sample CSV file (e.g., a
dataset from Kaggle).
– Use the assistant to perform tasks such as data cleaning, visualization, and
hypothesis testing.
– Document your process and findings, noting any challenges or improve-
ments needed.
Exercise 3—Create a Custom Action
Objective—Extend a GPT assistant with a custom action using a FastAPI service.
Tasks:
– Follow the steps to create a FastAPI service that provides a specific function,
such as fetching a list of daily tasks.
– Generate the OpenAPI specification for the service, and deploy it locally
using ngrok.
– Configure a new assistant to use this custom action, ensuring it connects cor-
rectly to the FastAPI endpoint.
– Test the assistant by asking it to perform the action and verify the output.

66
CHAPTER 3
Engaging GPT assistants
Exercise 4—File Upload Knowledge Assistant
Objective—Build an assistant with specialized knowledge from uploaded
documents.
Tasks:
– Select a freely available e-book or a collection of documents related to a spe-
cific topic (e.g., classic literature, technical manuals).
– Upload these files to a new GPT assistant, and configure the assistant to act
as an expert on the uploaded content.
– Create a series of prompts to test the assistant’s ability to reference and sum-
marize the information from the documents.
– Evaluate the assistant’s performance, and make any necessary adjustments to
improve its accuracy and helpfulness.
Exercise 5—Publish and Share Your Assistant
Objective—Publish your GPT assistant to the GPT Store and share it with others.
Tasks:
– Finalize the configuration and testing of your assistant to ensure it works as
intended.
– Write a compelling description, and create an appropriate logo for your
assistant.
– Choose the correct category, and set up any necessary links to your social
media or GitHub repository.
– Publish the assistant to the GPT Store, and share the link with friends or
colleagues.
– Gather feedback from users, and refine the assistant based on their input to
improve its usability and functionality.
Summary
The OpenAI GPT Assistants platform enables building and deploying AI agents
through the ChatGPT UI, focusing on creating engaging and functional assis-
tants.
You can use GPT’s code interpretation capabilities to perform data analysis on
user-uploaded CSV files, enabling assistants to function as data scientists.
Assistants can be extended with custom actions, allowing integration with exter-
nal services via API endpoints. This includes generating FastAPI services and
their corresponding OpenAPI specifications.
Assistants can be enriched with specialized knowledge through file uploads,
allowing them to act as authoritative sources on specific texts or documents.
Commercializing your GPT involves publishing it to the GPT Store, where you
can share and market your assistant to a broader audience.

67
Summary
Building a functional assistant involves iterating through design prompts, defin-
ing a clear persona, setting rules, and ensuring the assistant’s output aligns with
user expectations.
Creating custom actions requires understanding and implementing OpenAPI
specifications, deploying services locally using tools such as ngrok, and connect-
ing these services to your assistant.
Knowledge assistants can handle various tasks, from searching and comparing
documents to generating new content based on their knowledge base.
Publishing assistants require careful consideration of resource usage, user expe-
rience, and economic factors to ensure their effectiveness and sustainability for
public use.
The GPT Store, available to ChatGPT Plus subscribers, is a valuable platform
for learning and gaining proficiency in building AI assistants, with the potential
for future profit-sharing opportunities.

68
Exploring
multi-agent systems
Now let’s take a journey from AutoGen to CrewAI, two well-established multi-
agent platforms. We’ll start with AutoGen, a Microsoft project that supports mul-
tiple agents and provides a studio for working with them. We’ll explore a project
from Microsoft called AutoGen, which supports multiple agents but also provides
a studio to ease you into working with agents. From there, we’ll get more hands-
on coding of AutoGen agents to solve tasks using conversations and group chat
collaborations.
Then, we’ll transition to CrewAI, a self-proposed enterprise agentic system that
takes a different approach. CrewAI balances role-based and autonomous agents that
This chapter covers
Building multi-agent systems using AutoGen
Studio
Building a simple multi-agent system
Creating agents that can work collaboratively over
a group chat
Building an agent crew and multi-agent systems
using CrewAI
Extending the number of agents and exploring
processing patterns with CrewAI

69
4.1
Introducing multi-agent systems with AutoGen Studio
can be sequentially or hierarchically flexible task management systems. We’ll explore
how CrewAI can solve diverse and complex problems.
Multi-agent systems incorporate many of the same tools single-agent systems use
but benefit from the ability to provide outside feedback and evaluation to other
agents. This ability to support and criticize agent solutions internally gives multi-agent
systems more power. We’ll explore an introduction to multi-agent systems, beginning
with AutoGen Studio in the next section.
4.1
Introducing multi-agent systems with AutoGen Studio
AutoGen Studio is a powerful tool that employs multiple agents behind the scenes to
solve tasks and problems a user directs. This tool has been used to develop some of
the more complex code in this book. For that reason and others, it’s an excellent
introduction to a practical multi-agent system.
Figure 4.1 shows a schematic diagram of the agent connection/communication
patterns AutoGen employs. AutoGen is a conversational multi-agent platform because
communication is done using natural language. Natural language conversation seems
to be the most natural pattern for agents to communicate, but it’s not the only method,
as you’ll see later.
AutoGen supports various conversational patterns, from group and hierarchical to the
more common and simpler proxy communication. In proxy communication, one
agent acts as a proxy and directs communication to relevant agents to complete tasks.
A proxy is similar to a waiter taking orders and delivering them to the kitchen, which
cooks the food. Then, the waiter serves the cooked food.
AutoGen uses conversable agents, which
communicate through conversations.
Figure 4.1
How AutoGen agents communicate through conversations (Source: AutoGen)

70
CHAPTER 4
Exploring multi-agent systems
The basic pattern in AutoGen uses a UserProxy and one or more assistant
agents. Figure 4.2 shows the user proxy taking direction from a human and then
directing an assistant agent enabled to write code to perform the tasks. Each time
the assistant completes a task, the proxy agent reviews, evaluates, and provides feed-
back to the assistant. This iteration loop continues until the proxy is satisfied with
the results.
The benefit of the proxy is that it works to replace the required human feedback and
evaluation, and, in most cases, it does a good job. While it doesn’t eliminate the need
for human feedback and evaluation, it produces much more complete results overall.
And, while the iteration loop is time consuming, it’s time you could be drinking a cof-
fee or working on other tasks.
AutoGen Studio is a tool developed by the AutoGen team that provides a helpful
introduction to conversable agents. In the next exercise, we’ll install Studio and run
some experiments to see how well the platform performs. These tools are still in a
rapid development cycle, so if you encounter any problems, consult the documenta-
tion on the AutoGen GitHub repository.
4.1.1
Installing and using AutoGen Studio
Open the chapter_04 folder in Visual Studio Code (VS Code), create a local Python
virtual environment, and install the requirements.txt file. If you need assistance with
this, consult appendix B to install all of this chapter’s exercise requirements.
Open a terminal in VS Code (Ctrl-`, Cmd-`) pointing to your virtual environment,
and run AutoGen Studio using the command shown in listing 4.1. You’ll first need to
Human communicates
to the user proxy agent,
which communicates
to other agents.
Assistant agent
undertakes completion
of the direct tasks.
Evaluation and feedback
loop is formed between
the proxy and the assistant.
LLM conﬁgured to
write Python code
Figure 4.2
The user proxy agent and assistant agent communication (Source: AutoGen)

71
4.1
Introducing multi-agent systems with AutoGen Studio
define an environment variable for your OpenAI key. Because ports 8080 and 8081
are popular, and if you have other services running, change the port to 8082 or some-
thing you choose.
# set environment variable on Bash (Git Bash)
export OPENAI_API_KEY=”<your API key>”
# sent environment variable with PowerShell
$env:VAR_NAME =”<your API key>"
autogenstudio ui --port 8081
Navigate your browser to the AutoGen Studio interface shown in figure 4.3 (as of this
writing). While there may be differences, one thing is for sure: the primary interface
will still be chat. Enter a complex task that requires coding. The example used here is
Create a plot showing the popularity of the term GPT Agents in Google search.
Listing 4.1
Launching AutoGen Studio
Use the appropriate
command for your
terminal type.
Change the port if you expect or
experience a conflict on your machine.
Enter a task for the
agents to work on.
You can create new
sessions or review or
continue previous
sessions.
The Playground tab is where you interact
with agents. The Build tab is for creating new
agents and skills, and the Gallery tab is for
reviewing previous best output.
The proxy agent and assistant
agent will not work together
to complete the task.
Figure 4.3
Entering a task for the agents to work on in the AutoGen interface

72
CHAPTER 4
Exploring multi-agent systems
The agent assistant generates code snippets to perform or complete various subtasks
as the agents work together through the task in the example. The user proxy agent then
attempts to execute those code snippets and assesses the output. In many cases, prov-
ing the code runs and produces the required output is sufficient for the user proxy
agent to approve the task’s completion.
If you encounter any problems with the assistant agent requests, ask the proxy
agent to try a different method or another problem. This highlights a bigger problem
with agentic systems using packages or libraries that have expired and no longer work.
For this reason, it’s generally better to get agents to execute actions rather than build
code to perform actions as tools.
TIP
Executing AutoGen and AutoGen Studio using Docker is recommended,
especially when working with code that may affect the operating system. Docker
can isolate and virtualize the agents’ environment, thus isolating potentially
harmful code. Using Docker can help alleviate any secondary windows or
websites that may block the agent process from running.
Figure 4.4 shows the agent’s completion of the task. The proxy agent will collect any
generated code snippet, images, or other documents and append them to the message.
Generated code ﬁles and
other output will be
attached to the last message.
Reply with TERMINATE if you are
ﬁnished with this agent session. This
stop word is used to stop the session.
In this example, a Matplotlib
plot was generated in a new
window outside the browser.
Figure 4.4
The output after the agents complete the task

73
4.1
Introducing multi-agent systems with AutoGen Studio
You can also review the agent conversation by opening the Agent Messages expander.
In many cases, if you ask the agent to generate plots or applications, secondary win-
dows will open showing those results.
Amazingly, the agents will perform most tasks nicely and complete them well.
Depending on the complexity of the task, you may need to further iterate with the
proxy. Sometimes, an agent may only go so far to complete a task because it lacks the
required skills. In the next section, we’ll look at how to add skills to agents.
4.1.2
Adding skills in AutoGen Studio
Skills and tools, or actions, as we refer to them in this book, are the primary means by
which agents can extend themselves. Actions give agents the ability to execute code,
call APIs, or even further evaluate and inspect generated output. AutoGen Studio cur-
rently begins with just a basic set of tools to fetch web content or generate images.
NOTE
Many agentic systems employ the practice of allowing agents to code to
solve goals. However, we discovered that code can be easily broken, needs to be
maintained, and can change quickly. Therefore, as we’ll discuss in later chap-
ters, it’s better to provide agents with skills/actions/tools to solve problems.
In the following exercise scenario, we’ll add a skill/action to inspect an image using
the OpenAI vision model. This will allow the proxy agent to provide feedback if we ask
the assistant to generate an image with particular content.
With AutoGen Studio running, go to the Build tab and click Skills, as shown in
figure 4.5. Then, click the New Skill button to open a code panel where you can
copy–paste code to. From this tab, you can also configure models, agents, and agent
workflows.
Enter the code shown in listing 4.2 and also provided in the book’s source code as
describe_image.py. Copy and paste this code into the editor window, and then click
the Save button at the bottom.















74
CHAPTER 4
Exploring multi-agent systems
import base64
import requests
import os
def describe_image(image_path='animals.png') -> str:
"""
Uses GPT-4 Vision to inspect and describe the contents of the image.
:param input_path: str, the name of the PNG file to describe.
"""
api_key = os.environ['OPEN_API_KEY']
# Function to encode the image
def encode_image(image_path):
with open(image_path, "rb") as image_file:
return base64.b64encode(image_file.read()).decode('utf-8')
Listing 4.2
describe_image.py
Click existing skills
to see how they work.
Click to create a new skill.
You can also conﬁgure
other models, agents, and
workﬂows from here.
Figure 4.5
Steps to creating a new skill on the Build tab
Function to load and
encode the image as
a Base64 string

75
4.1
Introducing multi-agent systems with AutoGen Studio
# Getting the base64 string
base64_image = encode_image(image_path)
headers = {
"Content-Type": "application/json",
"Authorization": f"Bearer {api_key}"
}
payload = {
"model": "gpt-4-turbo",
"messages": [
{
"role": "user",
"content": [
{
"type": "text",
"text": "What’s in this image?"
},
{
"type": "image_url",
"image_url": {
"url": f"data:image/jpeg;base64,{base64_image}"
}
}
]
}
],
"max_tokens": 300
}
response = requests.post(
"https://api.openai.com/v1/chat/completions",
headers=headers,
json=payload)
return response.json()["choices"][0]["message"]
["content"]
The describe_image function uses the OpenAI GPT-4 vision model to describe what
is in the image. This skill can be paired with the existing generate_image skill as a
quality assessment. The agents can confirm that the generated image matches the
user’s requirements.
After the skill is added, it must be added to the specific agent workflow and agent
for use. Figure 4.6 demonstrates adding the new skill to the primary assistant agent in
the general or default agent workflow.
Now that the skill is added to the primary assistant, we can task the agent with cre-
ating a specific image and validating it using the new describe_image skill. Because
image generators notoriously struggle with correct text, we’ll create an exercise task to
do just that.
Enter the text shown in listing 4.3 to prompt the agents to create a book image
cover for this book. We’ll explicitly say that the text needs to be correct and insist that
the agent uses the new describe_image function to verify the image.
Including the
image string
along with the
JSON payload
Unpacking the response and
returning the content of the reply

76
CHAPTER 4
Exploring multi-agent systems
Please create a cover for the book GPT Agents In Action, use the
describe_image skill to make sure the title of the book is spelled
correctly on the cover
After the prompt is entered, wait for a while, and you may get to see some dialogue
exchanged about the image generation and verification process. In the end, though, if
everything works correctly, the agents will return with the results shown in figure 4.7.
Remarkably, the agent coordination completed the task in just a couple of itera-
tions. Along with the images, you can also see the various helper code snippets gener-
ated to assist with task completion. AutoGen Studio is impressive in its ability to
integrate skills that the agents can further adapt to complete some goal. The following
section will show how these powerful agents are implemented in code.
Listing 4.3
Prompting for a book cover
Select the General
Agent Workﬂow.
Select to edit the primary_assistant
at the bottom of the workﬂow panel.
Go to the Build tab, and then go to the Workﬂows tab.
Click add to add the new
describe_image skill.
Click OK to exit and return
to the Playground tab.
Figure 4.6
Configuring the primary_assistant agent with the new skill

77
4.2
Exploring AutoGen
4.2
Exploring AutoGen
While AutoGen Studio is a fantastic tool for understanding multi-agent systems, we
must look into the code. Fortunately, coding multiple agent examples with AutoGen is
simple and easy to run. We’ll cover the basic AutoGen setup in the next section.
4.2.1
Installing and consuming AutoGen
This next exercise will look at coding a basic multi-agent system that uses a user proxy
and conversable agent. Before we do that, though, we want to make sure AutoGen is
installed and configured correctly.
Open a terminal in VS Code, and run the entire chapter 4 install directions per
appendix B, or run the pip command in listing 4.4. If you’ve installed the require-
ments.txt file, you’ll also be ready to run AutoGen.
pip install pyautogen
Next, copy the chapter_04/OAI_CONFIG_LIST.example to OAI_CONFIG_LIST, remov-
ing .example from the file name. Then, open the new file in VS Code, and enter your
Listing 4.4
Installing AutoGen
Review the generated output ﬁles.
The agents generate additional
code to use the skills as needed.
Tho agents even enhance existing
skills to better complete the task.
After a couple iterations, the book cover
is generated with the correct text.
Figure 4.7
The generated file outputs from the agent work on the image generation task

78
CHAPTER 4
Exploring multi-agent systems
OpenAI or Azure configuration in the OAI_CONFIG_LIST file in listing 4.5. Fill in your
API key, model, and other details per your API service requirements. AutoGen will
work with any model that adheres to the OpenAI client. That means you can use local
LLMs via LM Studio or other services such as Groq, Hugging Face, and more.
[
{
"model": "gpt-4",
"api_key": "<your OpenAI API key here>",
"tags": ["gpt-4", "tool"]
},
{
"model": "<your Azure OpenAI deployment name>",
"api_key": "<your Azure OpenAI API key here>",
"base_url": "<your Azure OpenAI API base here>",
"api_type": "azure",
"api_version": "2024-02-15-preview"
}
]
Now, we can look at the code for a basic multi-agent chat using the out-of-the-box
UserProxy and ConversableAgent agents. Open autogen_start.py in VS Code,
shown in the following listing, and review the parts before running the file.
from autogen import ConversableAgent, UserProxyAgent, config_list_from_json
config_list = config_list_from_json(
env_or_file="OAI_CONFIG_LIST")
assistant = ConversableAgent(
"agent",
llm_config={"config_list": config_list})
user_proxy = UserProxyAgent(
"user",
code_execution_config={
"work_dir": "working",
"use_docker": False,
},
human_input_mode="ALWAYS",
is_termination_msg=lambda x: x.get("content", "")
.rstrip()
.endswith("TERMINATE"),
)
user_proxy.initiate_chat(assistant, message="write a solution
➥ for fizz buzz in one line?")
Listing 4.5
OAI_CONFIG_LIST
Listing 4.6
autogen_start.py
Select the model; GPT-4
is recommended.
Use the service key you
would typically use.
Select the model; GPT-4
is recommended.
Use the service
key you would
typically use.
Changing the base URL allows you to point to
other services, not just Azure OpenAI.
Loads your LLM
configuration from the
JSON file OAI_CONFIG_LIST
This agent talks
directly to the LLM.
This agent proxies
conversations from the
user to the assistant.
Setting the termination message
allows the agent to iterate.
A chat is initiated with the
assistant through the user_proxy
to complete a task.

79
4.2
Exploring AutoGen
Run the code by running the file in VS Code in the debugger (F5). The code in list-
ing 4.6 uses a simple task to demonstrate code writing. Listing 4.7 shows a few examples
to choose from. These coding tasks are also some of the author’s regular baselines to
assess an LLMs’ strength in coding.
write a Python function to check if a number is prime
code a classic sname game using Pygame
code a classic asteroids game in Python using Pygame
After the code starts in a few seconds, the assistant will respond to the proxy with a
solution. At this time, the proxy will prompt you for feedback. Press Enter, essentially
giving no feedback, and this will prompt the proxy to run the code to verify it operates
as expected.
Impressively, the proxy agent will even take cues to install required packages such
as Pygame. Then it will run the code, and you’ll see the output in the terminal or as a
new window or browser. You can play the game or use the interface if the code shelled
a new window/browser.
Note that the spawned window/browser won’t close on Windows and will require
exiting the entire program. To avoid this problem, run the code through Windows
Subsystem for Linux (WSL) or Docker. AutoGen explicitly recommends using Docker
for code execution agents, and if you’re comfortable with containers, this is a good
option.
Either way, after the proxy generates and runs the code, the working_dir folder
set earlier in listing 4.6 should now have a Python file with the code. This will allow
you to run the code at your leisure, make changes, or even ask for improvements, as
we’ll see. In the next section, we’ll look at how to improve the capabilities of the cod-
ing agents.
4.2.2
Enhancing code output with agent critics
One powerful benefit of multi-agent systems is the multiple roles/personas you can
automatically assign when completing tasks. Generating or helping to write code can be
an excellent advantage to any developer, but what if that code was also reviewed and
tested? In the next exercise, we’ll add another agent critic to our agent system to help
with coding tasks. Open autogen_coding_critic.py, as shown in the following listing.
from autogen import AssistantAgent, UserProxyAgent, config_list_from_json
config_list = config_list_from_json(env_or_file="OAI_CONFIG_LIST")
user_proxy = UserProxyAgent(
"user",
Listing 4.7
Simple coding task examples
Listing 4.8
autogen_coding_critic.py
To enjoy iterating
over these tasks, use
Windows Subsystem
for Linux (WSL) on
Windows, or use Docker.

80
CHAPTER 4
Exploring multi-agent systems
code_execution_config={
"work_dir": "working",
"use_docker": False,
"last_n_messages": 1,
},
human_input_mode="ALWAYS",
is_termination_msg=lambda x:
x.get("content", "").rstrip().endswith("TERMINATE"),
)
engineer = AssistantAgent(
name="Engineer",
llm_config={"config_list": config_list},
system_message="""
You are a profession Python engineer, known for your expertise in
software development.
You use your skills to create software applications, tools, and
games that are both functional and efficient.
Your preference is to write clean, well-structured code that is easy
to read and maintain.
""",
)
critic = AssistantAgent(
name="Reviewer",
llm_config={"config_list": config_list},
system_message="""
You are a code reviewer, known for your thoroughness and commitment
to standards.
Your task is to scrutinize code content for any harmful or
substandard elements.
You ensure that the code is secure, efficient, and adheres to best
practices.
You will identify any issues or areas for improvement in the code
and output them as a list.
""",
)
def review_code(recipient, messages, sender, config):
return f"""
Review and critque the following code.

{recipient.chat_messages_for_summary(sender)[-1]['content']}
"""
user_proxy.register_nested_chats(
[
{
"recipient": critic,
"message": review_code,
"summary_method": "last_msg",
"max_turns": 1,
}
],
trigger=engineer,
)
This time, the assistant is
given a system/persona
message.
A second assistant critic agent is
created with a background.
A custom function
helps extract the
code for review by
the critic.
A nested chat is
created between
the critic and the
engineer.

81
4.2
Exploring AutoGen
task = """Write a snake game using Pygame."""
res = user_proxy.initiate_chat(
recipient=engineer,
message=task,
max_turns=2,
summary_method="last_msg"
)
Run the autogen_coding_critic.py file in VS Code in debug mode, and watch the
dialog between the agents. This time, after the code returns, the critic will also be trig-
gered to respond. Then, the critic will add comments and suggestions to improve the
code.
Nested chats work well for supporting and controlling agent interactions, but we’ll
see a better approach in the following section. Before that though, we’ll review the
importance of the AutoGen cache in the next section.
4.2.3
Understanding the AutoGen cache
AutoGen can consume many tokens over chat iterations as a conversable multi-agent
platform. If you ask AutoGen to work through complex or novel problems, you may
even encounter token limits on your LLM; because of this, AutoGen supports several
methods to reduce token usage.
AutoGen uses caching to store progress and reduce token usage. Caching is
enabled by default, and you may have already encountered it. If you check your cur-
rent working folder, you’ll notice a .cache folder, as shown in figure 4.8. Caching
allows your agents to continue conversations if they get interrupted.
In code, you can control the cache folder for your agent’s run, as shown in listing 4.9.
By wrapping the initiate_chat call with the with statement, you can control the
The proxy agent initiates a
chat with a max delay and
explicit summary method.
The folder code
is output here.
Output code ﬁles are named
with temporary names.
A cache is denoted by the folder
and contains a SQLite database
of message history.
Figure 4.8
AutoGen cache
and working folders

82
CHAPTER 4
Exploring multi-agent systems
location and seed for the cache. This will allow you to save and return to long-running
AutoGen tasks in the future by just setting the cache_seed for the previous cache.
with Cache.disk(cache_seed=42) as cache:
res = user_proxy.initiate_chat(
recipient=engineer,
message=task,
max_turns=2,
summary_method="last_msg",
cache=cache,
)
This caching ability allows you to continue operations from the previous cache loca-
tion and captures previous runs. It can also be a great way to demonstrate and inspect
how an agent conversation generated the results. In the next section, we’ll look at
another conversational pattern in which AutoGen supports group chat.
4.3
Group chat with agents and AutoGen
One problem with chat delegation and nested chats or conversations is the convey-
ance of information. If you’ve ever played the telephone game, you’ve witnessed this
firsthand and experienced how quickly information can change over iterations. With
agents, this is certainly no different, and chatting through nested or sequential con-
versations can alter the task or even the desired result.
Figure 4.9 shows the difference between nested and collaborative group chats. We
used the nested chat feature in the previous section to build a nested agent chat. In
this section, we use the group chat to provide a more collaborative experience.
Open autogen_coding_group.py with relevant parts, as shown in listing 4.10. The
code is similar to the previous exercise but now introduces GroupChat and GroupChat-
Manager. The agents and messages are held with the group chat, similar to a messaging
channel in applications such as Slack or Discord. The chat manager coordinates the
message responses to reduce conversation overlap.
Listing 4.9
Setting the cache folder
The telephone game
The telephone game is a fun but educational game that demonstrates information
and coherence loss. Children form a line, and the first child receives a message only
they can hear. Then, in turn, the children verbally pass the message on to the next
child, and so on. At the end, the last child announces the message to the whole
group, which often isn’t even close to the same message.
To counter this, AutoGen provides a group chat, a mechanism by which agents par-
ticipate in a shared conversation. This allows agents to review all past conversations
and better collaborate on long-running and complex tasks.
Setting the seed_cache
denotes the individual
location.
Sets the cache as
a parameter

83
4.3
Group chat with agents and AutoGen
user_proxy = UserProxyAgent(
"user",
code_execution_config={
"work_dir": "working",
"use_docker": False,
"last_n_messages": 3,
},
human_input_mode="NEVER",
)
llm_config = {"config_list": config_list}
engineer = AssistantAgent(…
critic = AssistantAgent(…

groupchat = GroupChat(agents=[user_proxy,
engineer,
critic],
messages=[],
max_round=20)
manager = GroupChatManager(groupchat=groupchat,
llm_config=llm_config)
task = """Write a snake game using Pygame."""
Listing 4.10
autoget_coding_group.py (relevant sections)
User Proxy
Chat
Manager
Agents now collaborate
through the group chat
manager.
User Proxy
Message is sent
to the critic.
Engineer
Critic
Represents the nested
chat to the critic
Conversable Agents Nested Chat
Conversable Agents Group Chat
Messages go through the
group chat manager.
Critic responds with
suggested updates
and changes.
Proxy initiates chat
with engineer.
Engineer
Critic
Figure 4.9
The difference between nested and group chat for conversable agents
Human input is now
set to never, so no
human feedback.
Code omitted, but
consult changes to the
persona in the file
This object holds the
connection to all the
agents and stores the
messages.
The manager
coordinates the
conversation as a
moderator would.

84
CHAPTER 4
Exploring multi-agent systems
with Cache.disk(cache_seed=43) as cache:
res = user_proxy.initiate_chat(
recipient=manager,
message=task,
cache=cache,
)
Run this exercise, and you’ll see how the agents collaborate. The engineer will now
take feedback from the critic and undertake operations to address the critic’s sugges-
tions. This also allows the proxy to engage in all of the conversation.
Group conversations are an excellent way to strengthen your agents’ abilities as
they collaborate on tasks. However, they are also substantially more verbose and token
expensive. Of course, as LLMs mature, so do the size of their context token windows
and the price of token processing. As token windows increase, concerns over token
consumption may eventually go away.
AutoGen is a powerful multi-agent platform that can be experienced using a web
interface or code. Whatever your preference, this agent collaboration tool is an excel-
lent platform for building code or other complex tasks. Of course, it isn’t the only plat-
form, as you’ll see in the next section, where we explore a newcomer called CrewAI.
4.4
Building an agent crew with CrewAI
CrewAI is relatively new to the realm of multi-agent systems. Where AutoGen was
initially developed from research and then extended, CrewAI is built with enterprise
systems in mind. As such, the platform is more robust, making it less extensible in
some areas.
With CrewAI, you build a crew of agents to focus on specific areas of a task goal.
Unlike AutoGen, CrewAI doesn’t require the use of the user proxy agent but instead
assumes the agents only work among themselves.
Figure 4.10 shows the main elements of the CrewAI platform, how they connect
together, and their primary function. It shows a sequential-processing agent system
with generic researcher and writer agents. Agents are assigned tasks that may also include
tools or memory to assist them.
CrewAI supports two primary forms of processing: sequential and hierarchical. Fig-
ure 4.10 shows the sequential process by iterating across the given agents and their
associated tasks. In the next section, we dig into some code to set up a crew and
employ it to complete a goal and create a good joke.
4.4.1
Creating a jokester crew of CrewAI agents
CrewAI requires more setup than AutoGen, but this also allows for more control and
additional guides, which provide more specific context to guide the agents in com-
pleting the given task. This isn’t without problems, but it does offer more control than
AutoGen out of the box.
Open crewai_introduction.py in VS Code and look at the top section, as shown
in listing 4.11. Many settings are required to configure an agent, including the role,

85
4.4
Building an agent crew with CrewAI
goal, verboseness, memory, backstory, delegation, and even tools (not shown). In this
example, we’re using two agents: a senior joke researcher and a joke writer.
from crewai import Agent, Crew, Process, Task
from dotenv import load_dotenv
load_dotenv()
joke_researcher = Agent(
role="Senior Joke Researcher",
goal="Research what makes things funny about the following {topic}",
verbose=True,
memory=True,
backstory=(
"Driven by slapstick humor, you are a seasoned joke researcher"
"who knows what makes people laugh. You have a knack for finding"
Listing 4.11
crewai_introduction.py (agent section)
Crew
Tasks
Agents
Tools
Memory
Search
Call APIs
Access data
Conversational
Task speciﬁc
Semantic
Various forms of memory and
r
a
g
etrieval ugmented eneration (RAG)
patterns are supported.
research on this {topic}
write on this topic
Tools can be attached
to agents and tasks.
writer
goal:
backstory:
researcher
goal:
backstory:
Agents have a goal
and backstory as
their persona.
Processing can
be sequential
or hierarchical.
Sequential
processing
Figure 4.10
The composition of a CrewAI system
Creates the
agents and
provides them
a goal
verbose allows the
agent to emit output
to the terminal.
Supports the use of
memory for the agents
The backstory is the agent’s background—its persona.

86
CHAPTER 4
Exploring multi-agent systems
"the funny in everyday situations and can turn a dull moment into"
"a laugh riot."
),
allow_delegation=True,
)
joke_writer = Agent(
role="Joke Writer",
goal="Write a humourous and funny joke on the following {topic}",
verbose=True,
memory=True,
backstory=(
"You are a joke writer with a flair for humor. You can turn a"
"simple idea into a laugh riot. You have a way with words and"
"can make people laugh with just a few lines."
),
allow_delegation=False,
)
Moving down the code, we next see the tasks, as shown in listing 4.12. Tasks denote an
agent’s process to complete the primary system goal. They also link an agent to work
on a specific task, define the output from that task, and may include how it’s executed.
research_task = Task(
description=(
"Identify what makes the following topic:{topic} so funny."
"Be sure to include the key elements that make it humourous."
"Also, provide an analysis of the current social trends,"
"and how it impacts the perception of humor."
),
expected_output="A comprehensive 3 paragraphs long report
➥     on the latest jokes.",
agent=joke_researcher,
)
write_task = Task(
description=(
"Compose an insightful, humourous and socially aware joke on
{topic}."
"Be sure to include the key elements that make it funny and"
"relevant to the current social trends."
),
expected_output="A joke on {topic}.",
agent=joke_writer,
async_execution=False,
output_file="the_best_joke.md",
)
Listing 4.12
crewai_introduction.py (task section)
The agents can either be delegated to or
are allowed to delegate; True means they
can delegate.
Creates the
agents and
provides them
a goal
verbose allows the
agent to emit output
to the terminal.
Supports the use of
memory for the agents
The backstory is the
agent’s background—
its persona.
The Task description defines how
the agent will complete the task.
Explicitly defines the expected
output from performing the task
The
agent
assigned
to work
on the
task
The Task description defines how
the agent will complete the task.
Explicitly defines the expected
output from performing the task
If the agent should execute
asynchronously
Any output the agent will generate

87
4.4
Building an agent crew with CrewAI
Now, we can see how everything comes together as the Crew at the bottom of the file,
as shown in listing 4.13. Again, many options can be set when building the Crew,
including the agents, tasks, process type, memory, cache, maximum requests per min-
ute (max_rpm), and whether the crew shares.
crew = Crew(
agents=[joke_researcher, joke_writer],
tasks=[research_task, write_task],
process=Process.sequential,
memory=True,
cache=True,
max_rpm=100,
share_crew=True,
)
result = crew.kickoff(inputs={"topic": "AI engineer jokes"})
print(result)
When you’re done reviewing, run the file in VS Code (F5), and watch the terminal for
conversations and messages from the crew. As you can probably tell by now, the goal of
this agent system is to craft jokes related to AI engineering. Here are some of the fun-
nier jokes generated over a few runs of the agent system:
Why was the computer cold? It left Windows open.
Why don’t AI engineers play hide and seek with their algorithms? Because no
matter where they hide, the algorithms always find them in the “overfitting”
room!
What is an AI engineer’s favorite song? “I just called to say I love you . . . and to
collect more data for my voice recognition software.”
Why was the AI engineer broke? Because he spent all his money on cookies, but
his browser kept eating them.
Before you run more iterations of the joke crew, you should read the next section.
This section shows how to add observability to the multi-agent system.
4.4.2
Observing agents working with AgentOps
Observing a complex assemblage such as a multi-agent system is critical to understand-
ing the myriad of problems that can happen. Observability through application tracing
is a key element of any complex system, especially one engaged in enterprise use.
CrewAI supports connecting to a specialized agent operations platform appropri-
ately called AgentOps. This observability platform is generic and designed to support
Listing 4.13
crewai_introduction.py (crew section)
The agents assembled
into the crew
The tasks the agents
can work on
Defining how the agents
will interact
Whether the system should
use memory; needs to be set
if agents/tasks have it on
Whether the system
should use a cache,
similar to AutoGen
Maximum requests per minute
the system should limit itself to
Whether the crew should share
information, similar to group chat

88
CHAPTER 4
Exploring multi-agent systems
observability with any agent platform specific to LLM usage. Currently, no pricing or
commercialization details are available.
Connecting to AgentOps is as simple as installing the package, getting an API key,
and adding a line of code to your crew setup. This next exercise will go through the
steps to connect and run AgentOps.
Listing 4.14 shows installing the agentops package using pip. You can install the
package alone or as an additional component of the crewai package. Remember that
AgentOps can also be connected to other agent platforms for observability.
pip install agentops
or as an option with CrewAI
pip install crewai[agentops]
Before using AgentOps, you need to sign up for an API key. Following are the general
steps to sign up for a key at the time of writing:
1
Visit https://app.agentops.ai in your browser.
2
Sign up for an account.
3
Create a project, or use the default.
4
Go to Settings > Projects and API Keys.
5
Copy and/or generate a new API key; this will copy the key to your browser.
6
Paste the key to your .env file in your project.
After the API key is copied, it should resemble the example shown in the following
listing.
AGENTOPS_API_KEY="your API key"
Now, we need to add a few lines of code to the CrewAI script. Listing 4.16 shows the
additions as they are added to the crewai_agentops.py file. When creating your own
scripts, all you need to do is add the agentops package and initialize it when using
CrewAI.
import agentops
from crewai import Agent, Crew, Process, Task
from dotenv import load_dotenv
load_dotenv()
agentops.init()
Listing 4.14
Installing AgentOps
Listing 4.15
env.: Adding an AgentOps key
Listing 4.16
crewai_agentops.py (AgentOps additions)
The addition of the
required package
Make sure to initialize the
package after the environment
variables are loaded.

89
4.4
Building an agent crew with CrewAI
Run the crewai_agentops.py file in VS Code (F5), and watch the agents work as before.
However, you can now go to the AgentOps dashboard and view the agent interactions
at various levels.
Figure 4.11 shows the dashboard for running the joke crew to create the best joke.
Several statistics include total duration, the run environment, prompt and completion
tokens, LLM call timings, and estimated cost. Seeing the cost can be both sobering
and indicative of how verbose agent conversations can become.
The AgentOps platform is an excellent addition to any agent platform. While it’s built
into CrewAI, it’s helpful that the observability could be added to AutoGen or other
frameworks. Another attractive thing about AgentOps is that it’s dedicated to observing
agent interactions and not transforming from a machine learning operations platform.
In the future, we’ll likely see the spawn of more agent observability patterns.
One benefit that can’t be overstated is the cost observation that an observability
platform can provide. Did you notice in figure 4.11 that creating a single joke costs a
little over 50 cents? Agents can be very powerful, but they can also become very costly,
and it’s essential to observe what those costs are in terms of practicality and commer-
cialization.
You can even track individual
LLM calls, actions, and tool use.
Prompts and replies are also
captured for all iterations.
Various statistics are captured with
respect to the entire agent conversation
sequence, including costs.
System information
is captured as well.
Figure 4.11
The AgentOps dashboard for running the joke crew

90
CHAPTER 4
Exploring multi-agent systems
In the last section of this chapter, we’ll return to CrewAI and revisit building
agents that can code games. This will provide an excellent comparison between the
capabilities of AutoGen and CrewAI.
4.5
Revisiting coding agents with CrewAI
A great way to compare capabilities between multi-agent platforms is to implement
similar tasks in a bot. In this next set of exercises, we’ll employ CrewAI as a game pro-
gramming team. Of course, this could be adapted to other coding tasks as well.
Open crewai_coding_crew.py in VS Code, and we’ll first review the agent section
in listing 4.17. Here, we’re creating a senior engineer, a QA engineer, and a chief QA
engineer with a role, goal, and backstory.
print("## Welcome to the Game Crew")
print("-------------------------------")
game = input("What is the game you would like to build?
➥ What will be the mechanics?\n")
senior_engineer_agent = Agent(
role="Senior Software Engineer",
goal="Create software as needed",
backstory=dedent(
"""
You are a Senior Software Engineer at a leading tech think tank.
Your expertise in programming in python. and do your best to
produce perfect code
"""
),
allow_delegation=False,
verbose=True,
)
qa_engineer_agent = Agent(
role="Software Quality Control Engineer",
goal="create prefect code, by analizing the code
➥ that is given for errors",
backstory=dedent(
"""
You are a software engineer that specializes in checking code
for errors. You have an eye for detail and a knack for finding
hidden bugs.
You check for missing imports, variable declarations, mismatched
brackets and syntax errors.
You also check for security vulnerabilities, and logic errors
"""
),
allow_delegation=False,
verbose=True,
)
Listing 4.17
crewai_coding_crew.py (agent section)
Allows the user
to input the
instructions for
their game

91
4.5
Revisiting coding agents with CrewAI
chief_qa_engineer_agent = Agent(
role="Chief Software Quality Control Engineer",
goal="Ensure that the code does the job that it is supposed to do",
backstory=dedent(
"""
You are a Chief Software Quality Control Engineer at a leading
tech think tank. You are responsible for ensuring that the code
that is written does the job that it is supposed to do.
You are responsible for checking the code for errors and ensuring
that it is of the highest quality.
"""
),
allow_delegation=True,
verbose=True,
)
Scrolling down in the file will display the agent tasks, as shown in listing 4.18. The task
descriptions and expected output should be easy to follow. Again, each agent has a
specific task to provide better context when working to complete the task.
code_task = Task(
description=f"""
You will create a game using python, these are the instructions:
Instructions
------------
{game}
You will write the code for the game using python.""",
expected_output="Your Final answer must be the
➥ full python code, only the python code and nothing else.",
agent=senior_engineer_agent,
)
qa_task = Task(
description=f"""You are helping create a game
➥ using python, these are the instructions:
Instructions
------------
{game}
Using the code you got, check for errors. Check for logic errors,
syntax errors, missing imports, variable declarations,
mismatched brackets,
and security vulnerabilities.""",
expected_output="Output a list of issues you found in the code.",
agent=qa_engineer_agent,
)
evaluate_task = Task(
description=f"""You are helping create a game
➥ using python, these are the instructions:
Instructions
------------
{game}
Listing 4.18
crewai_coding_crew.py (task section)
Only the chief QA engineer
can delegate tasks.
The game instructions
are substituted into the
prompt using Python
formatting.

92
CHAPTER 4
Exploring multi-agent systems
You will look over the code to insure that it is complete and
does the job that it is supposed to do. """,
expected_output="Your Final answer must be the
➥ corrected a full python code, only the python code and nothing else.",
agent=chief_qa_engineer_agent,
)
Finally, we can see how this comes together by going to the bottom of the file, as
shown in listing 4.19. This crew configuration is much like what we’ve seen before.
Each agent and task are added, as well as the verbose and process attributes. For this
example, we’ll continue to use sequential methods.
crew = Crew(
agents=[senior_engineer_agent,
qa_engineer_agent,
chief_qa_engineer_agent],
tasks=[code_task, qa_task, evaluate_task],
verbose=2,
process=Process.sequential,
)
# Get your crew to work!
result = crew.kickoff()
print("######################")
print(result)
When you run the VS Code (F5) file, you’ll be prompted to enter the instructions for
writing a game. Enter some instructions, perhaps the snake game or another game
you choose. Then, let the agents work, and observe what they produce.
With the addition of the chief QA engineer, the results will generally look better
than what was produced with AutoGen, at least out of the box. If you review the code,
you’ll see that it generally follows good patterns and, in some cases, may even include
tests and unit tests.
Before we finish the chapter, we’ll make one last change to the crew’s processing
pattern. Previously, we employed sequential processing, as shown in figure 4.10. Fig-
ure 4.12 shows what hierarchical processing looks like in CrewAI.
Adding this manager is a relatively simple process. Listing 4.20 shows the addi-
tional code changes to a new file that uses the coding crew in a hierarchical method.
Aside from importing a class for connecting to OpenAI from LangChain, the other
addition is adding this class as the crew manger, manager_llm.




Listing 4.19
crewai_coding_crew.py (crew section)
Process is sequential.
No additional context is
provided in the kickoff.

93
4.5
Revisiting coding agents with CrewAI
from langchain_openai import ChatOpenAI
crew = Crew(
agents=[senior_engineer_agent,
qa_engineer_agent,
chief_qa_engineer_agent],
tasks=[code_task, qa_task, evaluate_task],
verbose=2,
process=Process.hierarchical,
manager_llm=ChatOpenAI(
temperature=0, model="gpt-4"
Listing 4.20
crewai_hierarchy.py (crew manager sections)
Crew
Tasks
Agents
Tools
Memory
Search
Call APIs
Access data
Conversational
Task speciﬁc
Semantic
Tools can be attached
to agents and tasks.
research on this {topic}
write on this topic
writer
goal:
backstory:
researcher
goal:
backstory:
Agents have a goal
and backstory as
their persona.
crew
manager
Hierarchical processing
is coordinated through
a managing agent.
Various forms of memory and
r
a
g
etrieval ugmented eneration (RAG)
patterns are supported.
Figure 4.12
Hierarchical processing of agents coordinated through a crew manager
Imports the LLM
connector from
LangChain
You must set a crew
manager when selecting
hierarchical processing.
Sets the crew manager to
be the LLM connector

94
CHAPTER 4
Exploring multi-agent systems
),
)
Run this file in VS Code (F5). When prompted, enter a game you want to create. Try
using the same game you tried with AutoGen; the snake game is also a good baseline
example. Observe the agents work through the code and review it repeatedly for
problems.
After you run the file, you can also jump on AgentOps to review the cost of this
run. Chances are, it will cost over double what it would have without the agent man-
ager. The output will also likely not be significantly better. This is the trap of building
agent systems without understanding how quickly things can spiral.
An example of this spiral that often happens when agents continually iterate over
the same actions is frequently repeating tasks. You can view this problem in AgentOps,
as shown in figure 4.13, by viewing the Repeat Thoughts plot.
The Repeat Thoughts plot from AgentOps is an excellent way to measure the repeti-
tion your agent system encounters. Overly repetitive thought patterns typically
mean the agent isn’t being decisive enough and instead keeps trying to generate a
different answer. If you encounter this problem, you want to change the agents’ pro-
cessing patterns, tasks, and goals. You may even want to alter the system’s type and
number of agents.
Multi-agent systems are an excellent way to break up work in terms of work pat-
terns of jobs and tasks. Generally, the job role is allocated to an agent role/persona,
You must set a crew manager when
selecting hierarchical processing.
Figure 4.13
The repetition of thoughts as they occurred within an agent run
Plot indicates the repetition
of the same thoughts in an
agent interaction.
2
1.5
1
0.5

95
4.6
Exercises
and the tasks it needs to complete may be implicit, as in AutoGen, or more explicit, as
in CrewAI.
In this chapter, we covered many useful tools and platforms that you can use right
away to improve your work, life, and more. That completes our journey through multi-
agent platforms, but it doesn’t conclude our exploration and use of multiple agents,
as we’ll discover in later chapters.
4.6
Exercises
Use the following exercises to improve your knowledge of the material:
Exercise 1—Basic Agent Communication with AutoGen
Objective—Familiarize yourself with basic agent communications and setup in
AutoGen.
Tasks:
– Set up AutoGen Studio on your local machine, following the instructions
provided in this chapter.
– Create a simple multi-agent system with a user proxy and two assistant agents.
– Implement a basic task where the user proxy coordinates between the assis-
tant agents to generate a simple text output, such as summarizing a short
paragraph.
Exercise 2—Implementing Advanced Agent Skills in AutoGen Studio
Objective—Enhance agent capabilities by adding advanced skills.
Tasks:
– Develop and integrate a new skill into an AutoGen agent that allows it to
fetch and display real-time data from a public API (e.g., weather information
or stock prices).
– Ensure the agent can ask for user preferences (e.g., city for weather, type of
stocks) and display the fetched data accordingly.
Exercise 3—Role-Based Task Management with CrewAI
Objective—Explore role-based task management in CrewAI.
Tasks:
– Design a CrewAI setup where multiple agents are assigned specific roles
(e.g., data fetcher, analyzer, presenter).
– Configure a task sequence where the data fetcher collects data, the analyzer
processes the data, and the presenter generates a report.
– Execute the sequence and observe the flow of information and task delega-
tion among agents.
Exercise 4—Multi-Agent Collaboration in Group Chat Using AutoGen
Objective—Understand and implement a group chat system in AutoGen to facil-
itate agent collaboration.

96
CHAPTER 4
Exploring multi-agent systems
Tasks:
– Set up a scenario where multiple agents need to collaborate to solve a com-
plex problem (e.g., planning an itinerary for a business trip).
– Use the group chat feature to allow agents to share information, ask ques-
tions, and provide updates to each other.
– Monitor the agents’ interactions and effectiveness in collaborative problem
solving.
Exercise 5—Adding and Testing Observability with AgentOps in CrewAI
Objective—Implement and evaluate the observability of agents using AgentOps
in a CrewAI environment.
Tasks:
– Integrate AgentOps into a CrewAI multi-agent system.
– Design a task for the agents that involves significant computation or data
processing (e.g., analyzing customer reviews to determine sentiment trends).
– Use AgentOps to monitor the performance, cost, and output accuracy of the
agents. Identify any potential inefficiencies or errors in agent interactions.
Summary
AutoGen, developed by Microsoft, is a conversational multi-agent platform that
employs a variety of agent types, such as user proxies and assistant agents, to
facilitate task execution through natural language interactions.
AutoGen Studio acts as a development environment that allows users to create,
test, and manage multi-agent systems, enhancing the usability of AutoGen.
AutoGen supports multiple communication patterns, including group chats
and hierarchical and proxy communications. Proxy communication involves a
primary agent (proxy) that interfaces between the user and other agents to
streamline task completion.
CrewAI offers a structured approach to building multi-agent systems with a focus
on enterprise applications. It emphasizes role-based and autonomous agent func-
tionalities, allowing for flexible, sequential, or hierarchical task management.
Practical exercises in the chapter illustrate how to set up and use AutoGen Stu-
dio, including installing necessary components and running basic multi-agent
systems.
Agents in AutoGen can be equipped with specific skills to perform tasks such as
code generation, image analysis, and data retrieval, thereby broadening their
application scope.
CrewAI is distinguished by its ability to structure agent interactions more rigidly
than AutoGen, which can be advantageous in settings that require precise and
controlled agent behavior.
CrewAI supports integrating memory and tools for agents to consume through
task completion.

97
Summary
CrewAI supports integration with observability tools such as AgentOps, which
provides insights into agent performance, interaction efficiency, and cost
management.
AgentOps is an agent observability platform that can help you easily monitor
extensive agent interactions.

98
Empowering agents
with actions
In this chapter, we explore actions through the use of functions and how agents
can use them as well. We’ll start by looking at OpenAI function calling and then
quickly move on to another project from Microsoft called Semantic Kernel (SK),
which we’ll use to build and manage skills and functions for agents or as agents.
We’ll finish the chapter using SK to host our first agent system. This will be a
complete chapter with plenty of annotated code examples.
This chapter covers
How an agent acts outside of itself using actions
Defining and using OpenAI functions
The Semantic Kernel and how to use
semantic functions
Synergizing semantic and native functions
Instantiating a GPT interface with
Semantic Kernel

99
5.1
Defining agent actions
5.1
Defining agent actions
ChatGPT plugins were first introduced to provide a session with abilities, skills, or
tools. With a plugin, you can search the web or create spreadsheets or graphs. Plugins
provide ChatGPT with the means to extend the platform.
Figure 5.1 shows how a ChatGPT plugin works. In this example, a new movie rec-
ommender plugin has been installed in ChatGPT. When a user asks ChatGPT to rec-
ommend a new movie, the large language model (LLM) recognizes that it has a
plugin to manage that action. It then breaks down the user request into actionable
parameters, which it passes to the new movie recommender.
The recommender then scrapes a website showcasing new movies and appends that
information to a new prompt request to an LLM. With this information, the LLM
Calls the plugin/function
with parameters
Here are some new movies
you may like to see...
ChatGPT with a new
movie recommender
plugin
ChatGPT user
Conﬁrms the use of
the recommender plugin
GPT-4.5?
Plugin calls LLM to get a recommendation
for the list of new movies.
Plugin uses a service
to nd new movie.
ﬁ
Plugin replies with
recommended new movie.
New Movie Recommender
plugin (agent)
The plugin/agent
scrapes websites for
new movies.
Use External Tools
Enhances model capabilities.
Tactics include embeddings-based search, code execution, and access
to speci c functions.
ﬁ
Actions
Memory
Prompt Engineering Strategies
Can you recommend
a new movie?
A plugin may use the same,
different, or even multiple
LLMs.
Second, the plugin
uses an LLM to get
a recommendation.
First, the plugin scrapes
a site for a list of new
movies.
The LLM recognizes the
request for a plugin and
then extracts the input
parameters required for
the plugin.
Figure 5.1
How a ChatGPT plugin operates and how plugins and other external tools (e.g., APIs) align with the
Use External Tools prompt engineering strategy

100
CHAPTER 5
Empowering agents with actions
responds to the recommender, which passes this back to ChatGPT. ChatGPT then
responds to the user with the recommended request.
We can think of plugins as proxies for actions. A plugin generally encapsulates one
or more abilities, such as calling an API or scraping a website. Actions, therefore, are
extensions of plugins—they give a plugin its abilities.
AI agents can be considered plugins and consumers of plugins, tools, skills, and
other agents. Adding skills, functions, and tools to an agent/plugin allows it to exe-
cute well-defined actions—figure 5.2 highlights where agent actions occur and their
interaction with LLMs and other systems.
An agent action is an ability that allows it to use a function, skill, or tool. What gets
confusing is that different frameworks use different terminology. We’ll define an
action as anything an agent can do to establish some basic definitions.
ChatGPT plugins and functions represent an actionable ability that ChatGPT or an
agent system can use to perform additional actions. Now let’s examine the basis for
OpenAI plugins and the function definition.
1. The LLM recognizes the
request for a plugin/agent
and then extracts the input
parameters required to
activate the agent.
Calls the agent/plugin
with parameters
Agent System
ChatGPT user
GPT-4.5?
Agent replies with
recommended new movie.
New Movie Recommender
plugin (agent)
2. The agent adds the
information to a prompt
used to make a request
to an LLM.
The plugin/agent scrapes
websites for new movies.
3. The agent uses an action
to ﬁnd new movies.
An agent may use the
same, different, or
even multiple LLMs.
Chevron denotes an
agent action.
An agent action can be a
function or skill/tool prompt.
4. The agent system passes
the responses to the LLM
to summarize the results.
Can you recommend
a new movie?
Here are some new movies
you may like to see...
Conﬁrms the use of
a recommender plugin
Plugin calls LLM to get a
recommendation for the list of new movies.
Agent uses a function or
skill to nd new movie.
ﬁ
Figure 5.2
How an agent uses actions to perform external tasks

101
5.2
Executing OpenAI functions
5.2
Executing OpenAI functions
OpenAI, with the enablement of plugins, introduced a structure specification for defin-
ing the interface between functions/plugins an LLM could action. This specification is
becoming a standard that LLM systems can follow to provide actionable systems.
These same function definitions are now also being used to define plugins for
ChatGPT and other systems. Next, we’ll explore how to use functions directly with
an LLM call.
5.2.1
Adding functions to LLM API calls
Figure 5.3 demonstrates how an LLM recognizes and uses the function definition to
cast its response as the function call.
Listing 5.1 shows the details of an LLM API call using tools and a function definition.
Adding a function definition allows the LLM to reply regarding the function’s input
parameters. This means the LLM will identify the correct function and parse the rele-
vant parameters for the user’s request.
response = client.chat.completions.create(
model="gpt-4-1106-preview",
messages=[{"role": "system",
"content": "You are a helpful assistant."},
{"role": "user", "content": user_message}],
Listing 5.1
first_function.py (API call)
GPT-4
Make a request to
LLM using tools.
Conﬁrms the request matches
a particular function deﬁnition
Extracts parameters matching
the function de nition from the
ﬁ
original request
Request
Model: GPT-4
Messages:
System: you are a ...
User: please recommend a movie.
Parameters:
Temperature: .7
Max tokens: 256
Tools
"type": "function",
:
Replies with the tool name
(function) and input parameters
for the function
Tools represents plugins or
functions added to a request.
The LLM does not
execute the function.
If the LLM doesn’t
match any tools, it
will respond given
the expected prompt.
Figure 5.3
How a single LLM request, including tools, gets interpreted by an LLM

102
CHAPTER 5
Empowering agents with actions
temperature=0.7,
tools=[
{
"type": "function",
"function": {
"name": "recommend",
"description": "Provide a … topic.",
"parameters": {
"type": "object",
"properties": {
"topic": {
"type": "string",
"description":
"The topic,… for.",
},
"rating": {
"type": "string",
"description":
"The rating … given.",
"enum": ["good",
"bad",
"terrible"]
},
},
"required": ["topic"],
},
},
}
]
)
To see how this works, open Visual Studio Code (VS Code) to the book’s source code
folder: chapter_4/first_function.py. It’s a good practice to open the relevant chapter
folder in VS Code to create a new Python environment and install the requirements.txt
file. If you need assistance with this, consult appendix B.
Before starting, correctly set up an .env file in the chapter_4 folder with your
API credentials. Function calling is an extra capability provided by the LLM com-
mercial service. At the time of writing, this feature wasn’t an option for open
source LLM deployments.
Next, we’ll look at the bottom of the code in first_function.py, as shown in list-
ing 5.2. Here are just two examples of calls made to an LLM using the request previ-
ously specified in listing 5.1. Here, each request shows the generated output from
running the example.
user = "Can you please recommend me a time travel movie?"
response = ask_chatgpt(user)
print(response)
Listing 5.2
first_function.py (exercising the API)
New parameter called tools
Sets the type of tool to function
Provides an excellent
description of what
the function does
Defines the type of parameters
for input; an object represents
a JSON document.
Excellent
descriptions
for each input
parameter
You can even
describe in terms
of enumerations.
Previously
defined function

103
5.2
Executing OpenAI functions
###Output
Function(arguments='{"topic":"time travel movie"}',
name='recommend')
user = "Can you please recommend me a good time travel movie?"
response = ask_chatgpt(user)
print(response)
###Output
Function(arguments='{"topic":"time travel movie",
"rating":"good"}',
name='recommend')
Run the first_function.py Python script in VS Code using the debugger (F5) or the
terminal to see the same results. Here, the LLM parses the input request to match any
registered tools. In this case, the tool is the single function definition, that is, the rec-
ommended function. The LLM extracts the input parameters from this function and
parses those from the request. Then, it replies with the named function and desig-
nated input parameters.
NOTE
The actual function isn’t being called. The LLM only returns the sug-
gested function and the relevant input parameters. The name and parame-
ters must be extracted and passed into a function matching the signature to
act on the function. We’ll look at an example of this in the next section.
5.2.2
Actioning function calls
Now that we understand that an LLM doesn’t execute the function or plugin directly,
we can look at an example that executes the tools. Keeping with the recommender
theme, we’ll look at another example that adds a Python function for simple
recommendations.
Figure 5.4 shows how this simple example will work. We’ll submit a single request
that includes a tool function definition, asking for three recommendations. The LLM,
in turn, will reply with the three function calls with input parameters (time travel, rec-
ipe, and gift). The results from executing the functions are then passed back to the
LLM, which converts them back to natural language and returns a reply.
Now that we understand the example, open parallel_functions.py in VS Code.
Listing 5.3 shows the Python function that you want to call to give recommendations.







Returned in
the name of the
function to call
and the extracted
input parameters
Previously
defined function
Returned in the name of the function to
call and the extracted input parameters

104
CHAPTER 5
Empowering agents with actions
def recommend(topic, rating="good"):
if "time travel" in topic.lower():
return json.dumps({"topic": "time travel",
"recommendation": "Back to the Future",
"rating": rating})
elif "recipe" in topic.lower():
return json.dumps({"topic": "recipe",
"recommendation": "The best thing … ate.",
"rating": rating})
elif "gift" in topic.lower():
return json.dumps({"topic": "gift",
"recommendation": "A glorious new...",
"rating": rating})
else:
return json.dumps({"topic": topic,
"recommendation": "unknown"})
Next, we’ll look at the function called run_conversation, where all the work starts
with the request construction.
user = """Can you please make recommendations for the following:
1. Time travel movies
2. Recipes
Listing 5.3
parallel_functions.py (recommend function)
Listing 5.4
parallel_functions.py (run_conversation, request)
Make a request to
LLM using tools.
GPT
Request
Messages:
User: Can you please make
recommendations for the following:
1. Time travel movies
2. Recipes
3. Gifts
Tools: recommend function de nition
ﬁ
Conﬁrms the request matches a
particular function deﬁnition and
there are 3 calls to evaluate
Returns 3 tool calls to the function recommend
“
”
Creates 3 function replies, one
for each recommendation
Add results of function execution to
conversation history, and ask LLM to respond.
Execute functions.
Return results of all three recommendations
in natural language.
GPT
Returns the function
name and parameters
Could be the same
or different LLM
Figure 5.4
A sample request returns three tool function calls and then submits the results back to the LLM
to return a natural language response.
Checks to
see if the
string is
contained
within the
topic input
If no topic is
detected, returns
the default
Returns a
JSON object

105
5.2
Executing OpenAI functions
3. Gifts"""
messages = [{"role": "user", "content": user}]
tools = [
{
"type": "function",
"function": {
"name": "recommend",
"description":
"Provide a recommendation for any topic.",
"parameters": {
"type": "object",
"properties": {
"topic": {
"type": "string",
"description":
"The topic, … recommendation for.",
},
"rating": {
"type": "string",
"description": "The rating … was given.",
"enum": ["good", "bad", "terrible"]
},
},
"required": ["topic"],
},
},
}
]
Listing 5.5 shows the request being made, which we’ve covered before, but there are a
few things to note. This call uses a lower model such as GPT-3.5 because delegating
functions is a more straightforward task and can be done using older, cheaper, less
sophisticated language models.
response = client.chat.completions.create(
model="gpt-3.5-turbo-1106",
messages=messages,
tools=tools,
tool_choice="auto",
)
response_message = response.choices[0].message
At this point, after the API call, the response should hold the information for the
required function calls. Remember, we asked the LLM to provide us with three recom-
mendations, which means it should also provide us with three function call outputs, as
shown in the following listing.

Listing 5.5
parallel_functions.py (run_conversation, API call)
The user message
asks for three
recommendations.
Note that there is no
system message.
Adds the function
definition to the tools
part of the request
LLMs that delegate to functions
can be simpler models.
Adds the messages and tools definitions
auto is the default.
The returned message
from the LLM

106
CHAPTER 5
Empowering agents with actions
tool_calls = response_message.tool_calls
if tool_calls:
available_functions = {
"recommend": recommend,
}
# Step 4: send the info for each function call and function response to
the model
for tool_call in tool_calls:
function_name = tool_call.function.name
function_to_call = available_functions[function_name]
function_args = json.loads(tool_call.function.arguments)
function_response = function_to_call(
topic=function_args.get("topic"),
rating=function_args.get("rating"),
)
messages.append(
{
"tool_call_id": tool_call.id,
"role": "tool",
"name": function_name,
"content": function_response,
}
)  # extend conversation with function response
second_response = client.chat.completions.create(
model="gpt-3.5-turbo-1106",
messages=messages,
)
return second_response.choices[0].message.content
The tool call outputs and the calls to the recommender function results are appended
to the messages. Notice how messages now also contain the history of the first call.
This is then passed back to the LLM to construct a reply in natural language.
Debug this example in VS Code by pressing the F5 key with the file open. The fol-
lowing listing shows the output of running parallel_functions.py.
Here are some recommendations for you:
1. Time travel movies: "Back to the Future"
2. Recipes: "The best thing you ever ate."
3. Gifts: "A glorious new..." (the recommendation was cut off, so I
couldn't provide the full recommendation)
I hope you find these recommendations helpful! Let me know if you need
more information.
This completes this simple demonstration. For more advanced applications, the func-
tions could do any number of things, from scraping websites to calling search engines
to completing far more complex tasks.
Listing 5.6
parallel_functions.py (run_conversation, tool_calls)
Listing 5.7
parallel_functions.py (output)
If the response contains
tool calls, execute them.
Only one function but
could contain several
Loops through the calls and replays
the content back to the LLM
Executes the recommend
function from extracted
parameters
Appends the results of
each function call to the
set of messages
Sends another request
to the LLM with updated
information and returns
the message reply

107
5.3
Introducing Semantic Kernel
Functions are an excellent way to cast outputs for a particular task. However, the
work of handling functions or tools and making secondary calls can be done in a
cleaner and more efficient way. The following section will uncover a more robust sys-
tem of adding actions to agents.
5.3
Introducing Semantic Kernel
Semantic Kernel (SK) is another open source project from Microsoft intended to help
build AI applications, which we call agents. At its core, the project is best used to
define actions, or what the platform calls semantic plugins, which are wrappers for skills
and functions.
Figure 5.5 shows how the SK can be used as a plugin and a consumer of OpenAI
plugins. The SK relies on the OpenAI plugin definition to define a plugin. That way, it
can consume and publish itself or other plugins to other systems.
An OpenAI plugin definition maps precisely to the function definitions in listing 5.4.
This means that SK is the orchestrator of API tool calls, aka plugins. That also means
that SK can help organize multiple plugins with a chat interface or an agent.
NOTE
The team at SK originally labeled the functional modules as skills. How-
ever, to be more consistent with OpenAI, they have since renamed skills to
LLM
Interface is de ned like an OpenAI plugin.
ﬁ
Semantic
Kernel
Interface as an OpenAI plugin
ChatGPT
Plugins (Semantic Skills and Native Functions)
Math Plugin
(native function)
Recommend Plugin
(semantic function)
Get Movies Plugin
(native plugin)
Please recommend
a movie.
Can be consumed as a plugin
and also consumes plugins
Requests can be made
directly to the kernel
The kernel itself can be
registered as a plugin.
LLM
Can be the same
LLM or different
Figure 5.5
How the Semantic Kernel integrates as a plugin and can also consume plugins

108
CHAPTER 5
Empowering agents with actions
plugins. What is more confusing is that the code still uses the term skills. There-
fore, throughout this chapter, we’ll use skills and plugins to mean the same thing.
SK is a useful tool for managing multiple plugins (actions for agents) and, as we’ll see
later, can also assist with memory and planning tools. For this chapter, we’ll focus on
the actions/plugins. In the next section, we look at how to get started using SK.
5.3.1
Getting started with SK semantic functions
SK is easy to install and works within Python, Java, and C#. This is excellent news as it also
allows plugins developed in one language to be consumed in a different language. How-
ever, you can’t yet develop a native function in one language and use it in another.
We’ll continue from where we left off for the Python environment using the
chapter_4 workspace in VS Code. Be sure you have a workspace configured if you
want to explore and run any examples.
Listing 5.8 shows how to install SK from a terminal within VS Code. You can also
install the SK extension for VS Code. The extension can be a helpful tool to create
plugins/skills, but it isn’t required.
pip uninstall semantic-kernel
git clone https://github.com/microsoft/semantic-kernel.git
cd semantic-kernel/python
pip install -e .
Once you finish the installation, open SK_connecting.py in VS Code. Listing 5.9
shows a demo of running an example quickly through SK. The example creates a chat
completion service using either OpenAI or Azure OpenAI.
import semantic_kernel as sk
selected_service = "OpenAI"
kernel = sk.Kernel()
service_id = None
if selected_service == "OpenAI":
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
api_key, org_id = sk.openai_settings_from_dot_env()
service_id = "oai_chat_gpt"
kernel.add_service(
OpenAIChatCompletion(
service_id=service_id,
ai_model_id="gpt-3.5-turbo-1106",
Listing 5.8
Installing Semantic Kernel
Listing 5.9
SK_connecting.py
Uninstalls any previous installations of SK
Clones the
repository to
a local folder
Changes to the source folder
Installs the editable package
from the source folder
Sets the service you’re using
(OpenAI or Azure OpenAI)
Creates the
kernel
Loads secrets
from the .env file
and sets them on
the chat service

109
5.3
Introducing Semantic Kernel
api_key=api_key,
org_id=org_id,
),
)
elif selected_service == "AzureOpenAI":
from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion
deployment, api_key, endpoint =
➥ sk.azure_openai_settings_from_dot_env()
service_id = "aoai_chat_completion"
kernel.add_service(
AzureChatCompletion(
service_id=service_id,
deployment_name=deployment,
endpoint=endpoint,
api_key=api_key,
),
)
#This function is currently broken
async def run_prompt():
result = await kernel.invoke_prompt(
➥ prompt="recommend a movie about
➥ time travel")
print(result)
# Use asyncio.run to execute the async function
asyncio.run(run_prompt())
###Output
One highly recommended time travel movie is "Back to the Future" (1985)
directed by Robert Zemeckis. This classic film follows the adventures of
teenager Marty McFly (Michael J. Fox)…
Run the example by pressing F5 (debugging), and you should see an output similar to
listing 5.9. This example demonstrates how a semantic function can be created with SK
and executed. A semantic function is the equivalent of a prompt template in prompt
flow, another Microsoft tool. In this example, we define a simple prompt as a function.
It’s important to note that this semantic function isn’t defined as a plugin. How-
ever, the kernel can create the function as a self-contained semantic element that
can be executed against an LLM. Semantic functions can be used alone or regis-
tered as plugins, as you’ll see later. Let’s jump to the next section, where we intro-
duce contextual variables.
5.3.2
Semantic functions and context variables
Expanding on the previous example, we can look at adding contextual variables to the
semantic function. This pattern of adding placeholders to prompt templates is one
we’ll review over and over. In this example, we look at a prompt template that has
placeholders for subject, genre, format, and custom.
Loads secrets
from the .env file
and sets them on
the chat service
Invokes the
prompt
Calls the function
asynchronously

110
CHAPTER 5
Empowering agents with actions
Open SK_context_variables.py in VS Code, as shown in the next listing. The
prompt is equivalent to setting aside a system and user section of the prompt.
#top section omitted…
prompt = """
system:
You have vast knowledge of everything and can recommend anything provided
you are given the following criteria, the subject, genre, format and any
other custom information.
user:
Please recommend a {{$format}} with the subject {{$subject}} and {{$genre}}.
Include the following custom information: {{$custom}}
"""
prompt_template_config = sk.PromptTemplateConfig(
template=prompt,
name="tldr",
template_format="semantic-kernel",
input_variables=[
InputVariable(
name="format",
description="The format to recommend",
is_required=True
),
InputVariable(
name="suject",
description="The subject to recommend",
is_required=True
),
InputVariable(
name="genre",
description="The genre to recommend",
is_required=True
),
InputVariable(
name="custom",
description="Any custom information [CA]
to enhance the recommendation",
is_required=True,
),
],
execution_settings=execution_settings,
)
recommend_function = kernel.create_function_from_prompt(
prompt_template_config=prompt_template_config,
function_name="Recommend_Movies",
plugin_name="Recommendation",
)
Listing 5.10
SK_context_variables.py
Defines a prompt
with placeholders
Configures a
prompt template
and input variable
definitions
Creates a kernel
function from
the prompt

111
5.4
Synergizing semantic and native functions
async def run_recommendation(
subject="time travel",
format="movie",
genre="medieval",
custom="must be a comedy"
):
recommendation = await kernel.invoke(
recommend_function,
sk.KernelArguments(subject=subject,
format=format,
genre=genre,
custom=custom),
)
print(recommendation)
# Use asyncio.run to execute the async function
asyncio.run(run_recommendation())
###Output
One movie that fits the criteria of being about time travel, set in a
medieval period, and being a comedy is "The Visitors" (Les Visiteurs)
from 1993. This French film, directed by Jean-Marie Poiré, follows a
knight and his squire who are transported to the modern era by a
wizard’s spell gone wrong.…
Go ahead and debug this example (F5), and wait for the output to be generated. That is
the basis for setting up SK and creating and exercising semantic functions. In the next
section, we move on to see how a semantic function can be registered as a skill/plugin.
5.4
Synergizing semantic and native functions
Semantic functions encapsulate a prompt/profile and execute through interaction with
an LLM. Native functions are the encapsulation of code that may perform anything
from scraping websites to searching the web. Both semantic and native functions can
register as plugins/skills in the SK kernel.
A function, semantic or native, can be registered as a plugin and used the same
way we registered the earlier function directly with our API calls. When a function is
registered as a plugin, it becomes accessible to chat or agent interfaces, depending on
the use case. The next section looks at how a semantic function is created and regis-
tered with the kernel.
5.4.1
Creating and registering a semantic skill/plugin
The VS Code extension for SK provides helpful tools for creating plugins/skills. In
this section, we’ll use the SK extension to create a plugin/skill and then edit the com-
ponents of that extension. After that, we’ll register and execute the plugin in the SK.
Figure 5.6 shows the process for creating a new skill within VS Code using the SK
extension. (Refer to appendix B for directions if you need to install this extension.)
You’ll then be given the option for the skill/plugin folder to place the function.
Always group functions that are similar together. After creating a skill, enter the name
Creates an asynchronous
function to wrap the
function call
Sets the
kernel
function
arguments

112
CHAPTER 5
Empowering agents with actions
and description of the function you want to develop. Be sure to describe the function
as if the LLM were going to use it.
You can see the completed skills and functions by opening the skills/plugin folder
and reviewing the files. We’ll follow the previously constructed example, so open the
skills/Recommender/Recommend_Movies folder, as shown in figure 5.7. Inside this folder
is a config.json file, the function description, and the semantic function/prompt in
a file called skprompt.txt.
Listing 5.11 shows the contents of the semantic function definition, also known as
the plugin definition. Note that the type is marked as completion and not of type
function because this is a semantic function. We would define a native function as a
type function.



1. Select the icon to create a
new semantic skill/plugin.
2. Select an existing skill
folder, or create a new one.
3. Name the function.
4. Then, provide a description.
Figure 5.6
The process of creating a new skill/plugin

113
5.4
Synergizing semantic and native functions
{
"schema": 1,
"type": "completion",
"description": "A function to recommend movies based on users list of
previously seen movies.",
"completion": {
"max_tokens": 256,
"temperature": 0,
"top_p": 0,
"presence_penalty": 0,
"frequency_penalty": 0
},
"input": {
"parameters": [
{
"name": "input",
"description": "The users list of previously seen movies.",
"defaultValue": ""
}
]
},
"default_backends": []
}
Next, we can look at the definition of the semantic function prompt, as shown in list-
ing 5.12. The format is a little different, but what we see here matches the earlier
examples using templating. This prompt recommends movies based on a list of mov-
ies the user has previously seen.
You are a wise movie recommender and you have been asked to recommend a
movie to a user.
You are provided a list of movies that the user has watched before.
You want to recommend a movie that the user has not watched before.
[INPUT]
Listing 5.11
Recommend_Movies/config.json
Listing 5.12
Recommend_Movies/skprompt.txt
The folder containing
the
/plugin
skill
An inner folder that holds
the plugin/skill deﬁnitions
Deﬁnes the function/plugin
description
Prompt that deﬁnes the
semantic function
Figure 5.7
The file and folder
structure of a semantic
function skill/plugin
Semantic functions are
functions of type completion.
We can also set the
completion parameters for
how the function is called.
Defines the parameters
input into the semantic
function

114
CHAPTER 5
Empowering agents with actions
{{$input}}
[END INPUT]
Now, we’ll dive into the code that loads the skill/plugin and executes it in a simple
example. Open the SK_first_skill.py file in VS Code. The following listing shows
an abridged version highlighting the new sections.
kernel = sk.Kernel()
plugins_directory = "plugins"
recommender = kernel.import_plugin_from_prompt_directory(
plugins_directory,
"Recommender",
)
recommend = recommender["Recommend_Movies"]
seen_movie_list = [
"Back to the Future",
"The Terminator",
"12 Monkeys",
"Looper",
"Groundhog Day",
"Primer",
"Donnie Darko",
"Interstellar",
"Time Bandits",
"Doctor Strange",
]
async def run():
result = await kernel.invoke(
recommend,
sk.KernelArguments(
settings=execution_settings, input=", ".join(seen_movie_list)
),
)
print(result)
asyncio.run(run())
###Output
Based on the list of movies you've provided, it seems you have an
interest in science fiction, time travel, and mind-bending narratives.
Given that you've watched a mix of classics and modern films in this
genre, I would recommend the following movie that you have not watched
before:
"Edge of Tomorrow" (also known as "Live Die Repeat: Edge of Tomorrow")…
Listing 5.13
SK_first_skill.py (abridged listing)
Loads the prompt from
the plugins folder
List of user’s previously
seen movies
Input is set to joined
list of seen movies.
Function is executed
asynchronously.

115
5.4
Synergizing semantic and native functions
The code loads the skill/plugin from the skills directory and the plugin folder.
When a skill is loaded into the kernel and not just created, it becomes a registered
plugin. That means it can be executed directly as is done here or through an LLM
chat conversation via the plugin interface.
Run the code (F5), and you should see an output like listing 5.13. We now have a
simple semantic function that can be hosted as a plugin. However, this function
requires users to input a complete list of movies they have watched. We’ll look at a
means to fix this by introducing native functions in the next section.
5.4.2
Applying native functions
As stated, native functions are code that can do anything. In the following example,
we’ll introduce a native function to assist the semantic function we built earlier.
This native function will load a list of movies the user has previously seen, from a file.
While this function introduces the concept of memory, we’ll defer that discussion until
chapter 8. Consider this new native function as any code that could virtually do anything.
Native functions can be created and registered using the SK extension. For this exam-
ple, we’ll create a native function directly in code to make the example easier to follow.
Open SK_native_functions.py in VS Code. We’ll start by looking at how the
native function is defined. A native function is typically defined within a class, which
simplifies managing and instantiating native functions.
class MySeenMoviesDatabase:
"""
Description: Manages the list of users seen movies.
"""
@kernel_function(
description="Loads a list of movies … user has already seen",
name="LoadSeenMovies",
)
def load_seen_movies(self) -> str:
try:
with open("seen_movies.txt", 'r') as file:
lines = [line.strip() for line in file.readlines()]
comma_separated_string = ', '.join(lines)
return comma_separated_string
except Exception as e:
print(f"Error reading file: {e}")
return None
With the native function defined, we can see how it’s used by scrolling down in the
file, as shown in the following listing.


Listing 5.14
SK_native_functions.py (MySeenMovieDatabase)
Provides a description
for the container class
Uses a decorator to
provide function
description and
name
The actual function
returns a list of movies in a
comma-separated string.
Loads seen
movies from
the text file

116
CHAPTER 5
Empowering agents with actions
plugins_directory = "plugins"
recommender = kernel.import_plugin_from_prompt_directory(
plugins_directory,
"Recommender",
)
recommend = recommender["Recommend_Movies"]
seen_movies_plugin = kernel.import_plugin_from_object(
MySeenMoviesDatabase(), "SeenMoviesPlugin"
)
load_seen_movies = seen_movies_plugin["LoadSeenMovies"]
async def show_seen_movies():
seen_movie_list = await load_seen_movies(kernel)
return seen_movie_list
seen_movie_list = asyncio.run(show_seen_movies())
print(seen_movie_list)
async def run():
result = await kernel.invoke(
recommend,
sk.KernelArguments(
settings=execution_settings,
input=seen_movie_list),
)
print(result)
asyncio.run(run())
###Output
The Matrix, The Matrix Reloaded, The Matrix Revolutions, The Matrix
Resurrections – output from print statement
Based on your interest in the "The Matrix" series, it seems you enjoy
science fiction films with a strong philosophical undertone and action
elements. Given that you've watched all
One important aspect to note is how the native function was imported into the kernel.
The act of importing to the kernel registers that function as a plugin/skill. This means
the function can be used as a skill from the kernel through other conversations or
interactions. We’ll see how to embed a native function within a semantic function in
the next section.
Listing 5.15
SK_native_functions (remaining code)
Loads the semantic function
as shown previously
Imports the skill
into the kernel and
registers the function
as a plugin
Loads the native
function
Executes the
function and returns
the list as a string
Wraps the
plugin call in an
asynchronous
function and
executes

117
5.4
Synergizing semantic and native functions
5.4.3
Embedding native functions within semantic functions
There are plenty of powerful features within SK, but one beneficial feature is the abil-
ity to embed native or semantic functions within other semantic functions. The follow-
ing listing shows how a native function can be embedded within a semantic function.
sk_prompt = """
You are a wise movie recommender and you have been asked to recommend a
movie to a user.
You have a list of movies that the user has watched before.
You want to recommend a movie that
the user has not watched before.
Movie List: {{MySeenMoviesDatabase.LoadSeenMovies}}.
"""
The next example, SK_semantic_native_functions.py, uses inline native and seman-
tic functions. Open the file in VS Code, and the following listing shows the code to
create, register, and execute the functions.
prompt_template_config = sk.PromptTemplateConfig(
template=sk_prompt,
name="tldr",
template_format="semantic-kernel",
execution_settings=execution_settings,
)
recommend_function = kernel.create_function_from_prompt(
prompt_template_config=prompt_template_config,
function_name="Recommend_Movies",
plugin_name="Recommendation",
)
async def run_recommendation():
recommendation = await kernel.invoke(
recommend_function,
sk.KernelArguments(),
)
print(recommendation)
# Use asyncio.run to execute the async function
asyncio.run(run_recommendation())
###Output
Based on the list provided, it seems the user is a fan of the Matrix
franchise. Since they have watched all four existing Matrix movies, I
would recommend a…
Listing 5.16
SK_semantic_native_functions.py (skprompt)
Listing 5.17
SK_semantic_native_functions.py (abridged)
The exact
instruction text
as previous
The native function is referenced and identified
by class name and function name.
Creates the prompt template
config for the prompt
Creates an inline semantic
function from the prompt
Executes the semantic
function asynchronously

118
CHAPTER 5
Empowering agents with actions
Run the code, and you should see an output like listing 5.17. One important aspect to
note is that the native function is registered with the kernel, but the semantic function
is not. This is important because function creation doesn’t register a function.
For this example to work correctly, the native function must be registered with the
kernel, which uses the import_plugin function call—the first line in listing 5.17. How-
ever, the semantic function itself isn’t registered. An easy way to register the function
is to make it a plugin and import it.
These simple exercises showcase ways to integrate plugins and skills into chat or
agent interfaces. In the next section, we’ll look at a complete example demonstrating
adding a plugin representing a service or GPT interface to a chat function.
5.5
Semantic Kernel as an interactive service agent
In chapter 1, we introduced the concept of the GPT interface—a new paradigm in
connecting services and other components to LLMs via plugins and semantic layers.
SK provides an excellent abstraction for converting any service to a GPT interface.
Figure 5.8 shows a GPT interface constructed around an API service called The
Movie Database (TMDB; www.themoviedb.org). The TMDB site provides a free API that
exposes information about movies and TV shows.
To follow along with the exercises in this section, you must register for a free account
from TMDB and create an API key. Instructions for getting an API key can be found at
the TMDB website (www.themoviedb.org) or by asking a GPT-4 turbo or a more
recent LLM.
User
Web Interface
The Movie Database
www.themoviedb.org
API Interface
GPT Interface
Semantic Kernel
Chat Interface
Agent Interface
A user can access the site now in
three ways: web, chat, or agent.
SK acts as an abstraction
and tool to expose the
interface as a plugin.
This is the semantic
mapping of functions
to API endpoints.
This is the developer
API endpoint exposed
by the site.
Figure 5.8
This layer architecture diagram shows the role of a GPT interface and the Semantic
Kernel being exposed to chat or agent interfaces.

119
5.5
Semantic Kernel as an interactive service agent
Over the next set of subsections, we’ll create a GPT interface using an SK set of
native functions. Then, we’ll use the SK kernel to test the interface and, later in this
chapter, implement it as plugins into a chat function. In the next section, we look at
building a GPT interface against the TMDB API.
5.5.1
Building a semantic GPT interface
TMDB is an excellent service, but it provides no semantic services or services that can
be plugged into ChatGPT or an agent. To do that, we must wrap the API calls that
TMDB exposes in a semantic service layer.
A semantic service layer is a GPT interface that exposes functions through natural
language. As discussed, to expose functions to ChatGPT or other interfaces such as
agents, they must be defined as plugins. Fortunately, SK can create the plugins for us
automatically, given that we write our semantic service layer correctly.
A native plugin or set of skills can act as a semantic layer. To create a native plugin,
create a new plugin folder, and put a Python file holding a class containing the set of
native functions inside that folder. The SK extension currently doesn’t do this well, so
manually creating the module works best.
Figure 5.9 shows the structure of the new plugin called Movies and the semantic
service layer called tmdb.py. For native functions, the parent folder’s name (Movies) is
used in the import.
Open the tmdb.py file in VS Code, and look at the top of the file, as shown in listing
5.18. This file contains a class called TMDbService, which exposes several functions
that map to API endpoint calls. The idea is to map the various relevant API function
calls in this semantic service layer. This will expose the functions as plugins for a chat
or agent interface.
from semantic_kernel.functions import kernel_funct
import requests
import inspect
def print_function_call():
#omitted …
Listing 5.18
tmdb.py (top of file)
Parent skills folder
Name of the plugin folder
File/module containing the class
and set of native functions
Figure 5.9
The folder and file
structure of the TMDB plugin
Prints the calls to
the functions for
debugging

120
CHAPTER 5
Empowering agents with actions
class TMDbService:
def __init__(self):
# enter your TMDb API key here
self.api_key = "your-TMDb-api-key"
@kernel_function(
description="Gets the movie genre ID for a given genre name",
name="get_movie_genre_id",
input_description="The movie genre name of the genre_id to get",
)
def get_movie_genre_id(self, genre_name: str) -> str:
print_function_call()
base_url = "https://api.themoviedb.org/3"
endpoint = f"{base_url}/genre/movie/list
➥ ?api_key={self.api_key}&language=en-US"
response = requests.get(endpoint)
if response.status_code == 200:
genres = response.json()['genres']
for genre in genres:
if genre_name.lower() in genre['name'].lower():
return str(genre['id'])
return None
The bulk of the code for the TMDbService and the functions to call the TMDB end-
points was written with the help of GPT-4 Turbo. Then, each function was wrapped
with the sk_function decorator to expose it semantically.
A few of the TMDB API calls have been mapped semantically. Listing 5.19 shows
another example of a function exposed to the semantic service layer. This function
pulls a current top 10 list of movies playing for a particular genre.
@kernel_function(
description="””
Gets a list of currently playing movies for a given genre””",
name="get_top_movies_by_genre",
input_description="The genre of the movies to get",
)
def get_top_movies_by_genre(self, genre: str) -> str:
print_function_call()
genre_id = self.get_movie_genre_id(genre)
if genre_id:
base_url = "https://api.themoviedb.org/3
playing_movies_endpoint = f"{base_url}/movie/now_playing?
➥ api_key={self.api_key}&language=en-US"
response = requests.get(
playing_movies_endpoint)
if response.status_code != 200:
return ""
Listing 5.19
tmdb.py (get_top_movies_by_genre)
Top-level service
and decorator
used to describe
the function (good
descriptions are
important)
Function wrapped in
semantic wrapper;
should return str
Calls the API endpoint, and,
if good (code 200), checks
for matching genre
Found the genre,
returns the id
Decorates the function
with descriptions
Finds the genre
id for the given
genre name
Gets a list of currently
playing movies

121
5.5
Semantic Kernel as an interactive service agent
playing_movies = response.json()['results'
for movie in playing_movies:
movie['genre_ids'] = [str(genre_id)
➥ for genre_id in movie['genre_ids']]
filtered_movies = [movie for movie
➥ in playing_movies if genre_id
➥ in movie['genre_ids']][:10]
results = ", ".join([movie['title'] for movie in
filtered_movies])
return results
else:
return ""
Look through the various other API calls mapped semantically. As you can see, there
is a well-defined pattern for converting API calls to a semantic service. Before we run
the full service, we’ll test each of the functions in the next section.
5.5.2
Testing semantic services
In a real-world application, you’ll likely want to write a complete set of unit or integra-
tion tests for each semantic service function. We won’t do that here; instead, we’ll
write a quick helper script to test the various functions.
Open test_tmdb_service.py in VS Code, and review the code, as shown in listing
5.20. You can comment and uncomment any functions to test them in isolation. Be
sure to have only one function uncommented at a time.
import semantic_kernel as sk
from plugins.Movies.tmdb import TMDbService
async def main():
kernel = sk.Kernel()
tmdb_service = kernel.import_plugin_from_object
➥ (TMDbService(), "TMDBService")
print(
await tmdb_service["get_movie_genre_id"](
kernel, sk.KernelArguments(
genre_name="action")
)
)
print(
await tmdb_service["get_tv_show_genre_id"](
kernel, sk.KernelArguments(
genre_name="action")
)
)
print(
await tmdb_service["get_top_movies_by_genre"](
kernel, sk.KernelArguments(
Listing 5.20
test_tmdb_service.py
Converts
genre_ids
to strings
Checks to see
if the genre id
matches movie
genres
Instantiates
the kernel
Imports the
plugin service
Inputs parameter
to functions,
when needed
Executes and
tests the various
functions
Inputs parameter
to functions,
when needed
Executes and
tests the various
functions

122
CHAPTER 5
Empowering agents with actions
genre_name="action")
)
)
print(
await tmdb_service["get_top_tv_shows_by_genre"](
kernel, sk.KernelArguments(
genre_name="action")
)
)
print(await tmdb_service["get_movie_genres"](
kernel, sk.KernelArguments()))
print(await tmdb_service["get_tv_show_genres"](
kernel, sk.KernelArguments()))
# Run the main function
if __name__ == "__main__":
import asyncio
asyncio.run(main())
###Output
Function name: get_top_tv_shows_by_genre
Arguments:
self = <skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0>
genre = action
Function name: get_tv_show_genre_id
Arguments:
self = <skills.Movies.tmdb.TMDbService object at 0x00000159F52090C0>
genre_name = action
Arcane, One Piece, Rick and Morty, Avatar: The Last Airbender, Fullmetal
Alchemist: Brotherhood, Demon Slayer: Kimetsu no Yaiba, Invincible,
Attack on Titan, My Hero Academia, Fighting Spirit, The Owl House
The real power of SK is shown in this test. Notice how the TMDbService class is imported
as a plugin, but we don’t have to define any plugin configurations other than what we
already did? By just writing one class that wrapped a few API functions, we’ve exposed
part of the TMDB API semantically. Now, with the functions exposed, we can look at
how they can be used as plugins for a chat interface in the next section.
5.5.3
Interactive chat with the semantic service layer
With the TMDB functions exposed semantically, we can move on to integrating them
into a chat interface. This will allow us to converse naturally in this interface to get var-
ious information, such as current top movies.
Open SK_service_chat.py in VS Code. Scroll down to the start of the new section
of code that creates the functions, as shown in listing 5.21. The functions created here
are now exposed as plugins, except we filter out the chat function, which we don’t
want to expose as a plugin. The chat function here allows the user to converse directly
with the LLM and shouldn’t be a plugin.
Inputs parameter
to functions,
when needed
Executes and tests the various functions
Executes and tests
the various functions
Executes main
asynchronously
Calls print
function details
to notify when
the function is
being called

123
5.5
Semantic Kernel as an interactive service agent
system_message = "You are a helpful AI assistant."
tmdb_service = kernel.import_plugin_from_object(
TMDbService(), "TMDBService")
# extracted section of code
execution_settings = sk_oai.OpenAIChatPromptExecutionSettings(
service_id=service_id,
ai_model_id=model_id,
max_tokens=2000,
temperature=0.7,
top_p=0.8,
tool_choice="auto",
tools=get_tool_call_object(
kernel, {"exclude_plugin": ["ChatBot"]}),
)
prompt_config = sk.PromptTemplateConfig.from_completion_parameters(
max_tokens=2000,
temperature=0.7,
top_p=0.8,
function_call="auto",
chat_system_prompt=system_message,
)
prompt_template = OpenAIChatPromptTemplate(
"{{$user_input}}", kernel.prompt_template_engine, prompt_config
)
history = ChatHistory()
history.add_system_message("You recommend movies and TV Shows.")
history.add_user_message("Hi there, who are you?")
history.add_assistant_message(
"I am Rudy, the recommender chat bot. I'm trying to figure out what
people need."
)
chat_function = kernel.create_function_from_prompt(
prompt_template_config=prompt_template,
plugin_name="ChatBot",
function_name="Chat",
)
Next, we can continue by scrolling in the same file to review the chat function, as
shown in the following listing.
async def chat() -> bool:
try:
user_input = input("User:> ")
except KeyboardInterrupt:
Listing 5.21
SK_service_chat.py (function setup)
Listing 5.22
SK_service_chat.py (chat function)
Imports the
TMDbService
as a plugin
Configures the
execution settings and
adds filtered tools
Configures
the prompt
configuration
Defines the input
template and takes full
strings as user input
Adds the chat history object
and populates some history
Creates the chat function
Input is taken
directly from the
terminal/console.

124
CHAPTER 5
Empowering agents with actions
print("\n\nExiting chat...")
return False
except EOFError:
print("\n\nExiting chat...")
return False
if user_input == "exit":
print("\n\nExiting chat...")
return False
arguments = sk.KernelArguments(
user_input=user_input,
history=("\n").join(
[f"{msg.role}: {msg.content}" for msg in history]),
)
result = await chat_completion_with_tool_call(
kernel=kernel,
arguments=arguments,
chat_plugin_name="ChatBot",
chat_function_name="Chat",
chat_history=history,
)
print(f"AI Agent:> {result}")
return True
Lastly, scroll down to the bottom of the file, and review the primary function. This is
the code that calls the chat function in a loop.
async def main() -> None:
chatting = True
context = kernel.create_new_context()
print("Welcome to your first AI Agent\
\n  Type 'exit' to exit.\
\n  Ask to get a list of currently playing movies by genre."
)
while chatting:
chatting, context = await chat(context)
if __name__ == "__main__":
asyncio.ru n(main())
Run the chat interface, run the file (F5), and then ask about movies or television
shows of a particular genre. An example conversation session is shown in listing 5.24.
This output shows how a request to list movies from two genres made the chat inter-
face make multiple calls to the get_top_movie_by_genre function.
Welcome to your first AI Agent
Type 'exit' to exit.
Listing 5.23
SK_service_chat.py (main function)
Listing 5.24
SK_service_chat.py (example conversation)
If the user types
exit, then exit
the chat.
Creates arguments to
pass to the function
Uses the utility
function to call
the function and
execute the tool
Introduction
to the user
Continues until
chatting is False
Calls the
chat function
asynchronously

125
5.6
Thinking semantically when writing semantic services
Ask to get a list of currently playing movies by genre.
User:> Input: can you give me list of the current top playing movies for
the action and comedy genres?
Function name: get_top_movies_by_genre
Arguments:
genre = action
Function name: get_movie_genre_id
Arguments:
genre_name = action
Function name: get_top_movies_by_genre
Arguments:
genre = comedy
Function name: get_movie_genre_id
Arguments:
genre_name = comedy
Agent:> Here are the current top-playing movies
for the action and comedy genres:
**Action:**
1. The Hunger Games: The Ballad of Songbirds & Snakes
2. Rebel Moon - Part One: A Child of Fire
3. Aquaman and the Lost Kingdom
4. Silent Night
5. The Family Plan
6. Freelance
7. Migration
8. Sound of Freedom
9. Godzilla Minus One
**Comedy:**
1. The Family Plan
2. Wonka
3. Freelance
4. Saltburn
5. Chicken Run: Dawn of the Nugget
6. Trolls Band Together
7. There's Something in the Barn
8. Migration
Please note that some movies may overlap in both genres, such as
"The Family Plan" and "Freelance
."
Be sure to explore the chat interface’s boundaries and what you can ask for from the
TMDB service. For example, try asking for a list of genres for movies or television
shows. This service is a good first try, but we can perhaps do better, as we’ll see in the
next section.
5.6
Thinking semantically when writing semantic services
Now we’ve seen an excellent demonstration of converting an API into a semantic ser-
vice interface. As it is, the functions return the titles of the top movies and television
shows currently playing. However, by just returning the titles, we’re limiting the ability
of the LLM to parse the results on its own.
LLM makes two calls to
get_top_movies_by_genre.
Internal
call to
get the
genre id
List of the top
current action
movies
List of the top
current comedy
movies

126
CHAPTER 5
Empowering agents with actions
Therefore, we’ll create a v2 version of TMDbService to correct this and return the
results as JSON strings. Open the file tmdb_v2.py in VS Code, and scroll down to the
get_top_movies_by_genre function.
def get_top_movies_by_genre(self, genre: str) -> str:
print_function_call()
genre_id = self.get_movie_genre_id(genre)
if genre_id:
#same code …
return json.dumps(filtered_movies)
else:
return ""
Now open SK_service_chat.py in VS Code, and comment and uncomment the line
shown in listing 5.26. This will then use version 2 of the TMDbService that outputs
results as full JSON documents in a single string.
#from skills.Movies.tmdb import TMDbService
from skills.Movies.tmdb_v2 import TMDbService
Rerun the SK_service_chat.py file in VS Code, and alter your query slightly, as
shown by the output in the following listing.
User:> get a list of currently playing movies for the
action genre and only return movies about space
Agent:> To find currently playing action movies that are specifically
about space, I will need to manually filter the provided list for those
that have space-related themes in their overview. Here's what fits that
criteria from the list:
1. **Rebel Moon - Part One: A Child of Fire**
- Release Date: 2023-12-15
- Overview: When a peaceful colony on the edge of the galaxy finds
itself threatened by the armies of the tyrannical Regent Balisarius,
they dispatch Kora, a young woman with a mysterious past, to seek out
warriors from neighboring planets to help them take a stand.
This is the only movie from the provided list that clearly mentions a
space-related theme in its overview. …
Listing 5.25
tmdb_v2.py (get_top_movies_by_genre)
Listing 5.26
SK_service_chat.py (modifying imports)
Listing 5.27
SK_service_chat.py (TMDb_v2 service output)
Now returns a
filtered list as
a JSON string
Comment out this line.
Uncomment this line to use
version 2 of the service.
New query asks to
include an additional
filter for space
The LLM calls the service and then
reviews the returned results that
match the filter.

127
5.7
Exercises
Because the semantic service functions now return the complete movie listing in
JSON, the LLM can apply additional filtering. This is the real power of semantic
services, allowing you to process the data through the LLM. We won’t see this power
by just returning a list of titles.
This last exercise demonstrated the change in mentality you need to make when
writing semantic service layers. Generally, you’ll typically want to return as much infor-
mation as possible. Returning more information takes advantage of the LLM abilities
to filter, sort, and transform data independently. In the next chapter, we’ll explore
building autonomous agents using behavior trees.
5.7
Exercises
Complete the following exercises to improve your knowledge of the material:
Exercise 1—Creating a Basic Plugin for Temperature Conversion
Objective—Familiarize yourself with creating a simple plugin for the OpenAI chat
completions API.
Tasks:
– Develop a plugin that converts temperatures between Celsius and Fahrenheit.
– Test the plugin by integrating it into a simple OpenAI chat session where
users can ask for temperature conversions.
Exercise 2—Developing a Weather Information Plugin
Objective—Learn to create a plugin that performs a unique task.
Tasks:
– Create a plugin for the OpenAI chat completions API that fetches weather
information from a public API.
– Ensure the plugin can handle user requests for current weather conditions
in different cities.
Exercise 3—Crafting a Creative Semantic Function
Objective—Explore the creation of semantic functions.
Tasks:
– Develop a semantic function that writes a poem or tells a children’s story
based on user input.
– Test the function in a chat session to ensure it generates creative and coher-
ent outputs.
Exercise 4—Enhancing Semantic Functions with Native Functions
Objective—Understand how to combine semantic and native functions.
Tasks:
– Create a semantic function that uses a native function to enhance its capabilities.
– For example, develop a semantic function that generates a meal plan and
uses a native function to fetch nutritional information for the ingredients.

128
CHAPTER 5
Empowering agents with actions
Exercise 5—Wrapping an Existing Web API with Semantic Kernel
Objective—Learn to wrap existing web APIs as semantic service plugins.
Tasks:
– Use SK to wrap a news API and expose it as a semantic service plugin in a
chat agent.
– Ensure the plugin can handle user requests for the latest news articles on
various topics.
Summary
Agent actions extend the capabilities of an agent system, such as ChatGPT. This
includes the ability to add plugins to ChatGPT and LLMs to function as proxies
for actions.
OpenAI supports function definitions and plugins within an OpenAI API ses-
sion. This includes adding function definitions to LLM API calls and under-
standing how these functions allow the LLM to perform additional actions.
The Semantic Kernel (SK) is an open source project from Microsoft that can be
used to build AI applications and agent systems. This includes the role of seman-
tic plugins in defining native and semantic functions.
Semantic functions encapsulate the prompt/profile template used to engage
an LLM.
Native functions encapsulate code that performs or executes an action using an
API or other interface.
Semantic functions can be combined with other semantic or native functions
and layered within one another as execution stages.
SK can be used to create a GPT interface over the top of API calls in a semantic
service layer and expose them as chat or agent interface plugins.
Semantic services represent the interaction between LLMs and plugins, as well as
the practical implementation of these concepts in creating efficient AI agents.

129
Building
autonomous assistants
Now that we’ve covered how actions extend the power/capabilities of agents, we
can look at how behavior trees can guide agentic systems. We’ll start by understand-
ing the basics of behavior trees and how they control robotics and AI in games.
We’ll return to agentic actions and examine how actions can be implemented
on the OpenAI Assistants platform using the GPT Assistants Playground project.
From there, we’ll look at how to build an autonomous agentic behavior tree (ABT)
using OpenAI assistants. Then, we’ll move on to understanding the need for con-
trols and guardrails on autonomous agents and using control barrier functions.
This chapter covers
Behavior trees for robotics and AI apps
GPT Assistants Playground and creating
assistants and actions
Autonomous control of agentic behavior trees
Simulating conversational multi-agent systems
via agentic behavior trees
Using back chaining to create behavior trees
for complex systems

130
CHAPTER 6
Building autonomous assistants
In the final section of the chapter, we’ll examine the use of the AgentOps platform
to monitor our autonomous behavior-driven agentic systems. This will be an exciting
chapter with several challenges. Let’s begin by jumping into the next section, which
introduces behavior trees.
6.1
Introducing behavior trees
Behavior trees are a long-established pattern used to control robotics and AI in games.
Rodney A. Brooks first introduced the concept in his “A Robust Layered Control Sys-
tem for a Mobile Robot” paper in 1986. This laid the groundwork for a pattern that
expanded on using the tree and node structure we have today.
If you’ve ever played a computer game with nonplayer characters (NPCs) or inter-
acted with advanced robotic systems, you’ve witnessed behavior trees at work. Figure 6.1
shows a simple behavior tree. The tree represents all the primary nodes: selector or
fallback nodes, sequence nodes, action nodes, and condition nodes.
Table 6.1 describes the functions and purpose of the primary nodes we’ll explore in
this book. There are other nodes and node types, and you can even create custom
nodes, but for now, we’ll focus on those in the table.
Table 6.1
The primary nodes used in behavior trees
Node
Purpose
Function
Type
Selector
(fallback)
This node works by selecting the first
child that completes successfully. It’s
often called the fallback node
because it will always fall back to the
last successful node that executed.
The node calls its children in
sequence and stops executing
when the first child succeeds.
When a child node succeeds, it will
return success; if no nodes suc-
ceed, it returns failure.
Composite
→
?
?
→
→
The root node can be any
composite node, such as
selector or sequence.
Execution ﬂows from
top to bottom and
then from left to right.
1
2
3
4
5
6
7
Figure 6.1
A simple behavior tree of eating an apple or a pear

131
6.1
Introducing behavior trees
The primary nodes in table 6.1 can provide enough functionality to handle numerous
use cases. However, understanding behavior trees initially can be daunting. You won’t
appreciate their underlying complexity until you start using them. Before we build
some simple trees, we want to look at execution in more detail in the next section.
6.1.1
Understanding behavior tree execution
Understanding how behavior trees execute is crucial to designing and implementing
behavior trees. Unlike most concepts in computer science, behavior trees operate in
terms of success and failure. When a node in a behavior tree executes, it will return
either success or failure; this even applies to conditions and selector nodes.
Behavior trees execute from top to bottom and left to right. Figure 6.2 shows
the process and what happens if a node fails or succeeds. In the example, the AI
the tree controls has an apple but no pear. In the first sequence node, a condition
checks if the AI has an apple. Because the AI doesn’t have an apple, it aborts the
sequence and falls back to the selector. The selector then selects its next child
node, another sequence, that checks if the AI has a pear, and because it does, the
AI eats the apple.
Sequence
This node executes all of its children
in sequence until one node fails or
they all complete successfully.
The node calls each of its children
in sequence regardless of whether
they fail or succeed. If all children
succeed, it returns success, and
failure if just one child fails.
Composite
Condition
Behavior trees don’t use Boolean
logic but rather success or failure
as a means of control. The condition
returns success if the condition is
true and false otherwise.
The node returns success or failure
based on a condition.
Task
Action
This is where the action happens.
The node executes and returns suc-
cess if successful or returns failure
otherwise.
Task
Decorator
They work by controlling the execu-
tion of child nodes. They are often
referred to as conditionals because
they can determine whether a node is
worth executing or safe to execute.
The node controls execution of
the child nodes. Decorators can
operate as control barrier functions
to block or prevent unwanted
behaviors.
Decorator
Parallel
This node executes all of its nodes in
parallel. Success or failure is con-
trolled by a threshold of the number
of children needed to succeed to
return success.
The node executes all of its child
nodes in sequence regardless of
the status of the nodes.
Composite
Table 6.1
The primary nodes used in behavior trees (continued)
Node
Purpose
Function
Type

132
CHAPTER 6
Building autonomous assistants
Behavior trees provide control over how an AI system will execute at a macro or micro
level. Regarding robotics, behavior trees will typically be designed to operate at the
micro level, where each action or condition is a small event, such as detecting the
apple. Conversely, behavior trees can also control more macro systems, such as NPCs
in games, where each action may be a combination of events, like attacking the player.
For agentic systems, behavior trees support controlling an agent or assistant at
your chosen level. We’ll explore controlling agents at the task and, in later chapters,
the planning levels. After all, with the power of LLMs, agents can construct their own
behavior tree.
Of course, several other forms of AI control could be used to control agentic sys-
tems. The next section will examine those different systems and compare them to
behavior trees.
6.1.2
Deciding on behavior trees
Numerous other AI control systems have benefits and are worth exploring in con-
trolling agentic systems. They can demonstrate the benefits of behavior trees and pro-
vide other options for specific use cases. The behavior tree is an excellent pattern, but
it isn’t the only one, and it’s worth learning about others.
Table 6.2 highlights several other systems we may consider for controlling AI sys-
tems. Each item in the table describes what the method does, its shortcomings, and its
possible application to agentic AI control.

The root node
executes according
to its composite type.
Sequence nodes execute
all child nodes in order; if
a node fails, the sequence
fails.
If the previous node in
a sequence fails, the
sequence is aborted.
In this example, the AI has the
pear, which returns success
and then eats the pear.
Node success/failure
ﬂows back to the
parent node.
Selector nodes execute
all children and return
success on the ﬁrst child
that returns success.
?
→
→
1
2
3
4
6
7
Figure 6.2
The execution process of a simple behavior tree

133
6.1
Introducing behavior trees
Table 6.2
Comparison of other AI control systems
Control name
Description
Shortcomings
Control agentic AI?
Finite state
machinea (FSM)
FSMs model AI using a set
of states and transitions
triggered by events or
conditions.
FSMs can become
unwieldy with increasing
complexity.
FSMs aren’t practical for
agents because they don’t
scale well.
Decision treeb
Decision trees use a
tree-like model of deci-
sions and their possible
consequences.
Decision trees can suffer
from overfitting and lack
generalization in complex
scenarios.
Decision trees can be
adapted and enhanced
with behavior trees.
Utility-based
systemb
Utility functions evaluate
and select the best action
based on the current
situation.
These systems require
careful design of utility
functions to balance
priorities.
This pattern can be
adopted within a behavior
tree.
Rule-based
systema
This set of if-then rules
define the behavior of
the AI.
These systems can
become cumbersome with
many rules, leading to
potential conflicts.
These aren’t very practical
when paired with agentic
systems powered by
LLMs.
Planning sys-
temc
Planning systems gener-
ate a sequence of actions
to achieve a specific goal
using planning algorithms.
These systems are com-
putationally expensive and
require significant domain
knowledge.
Agents can already imple-
ment such patterns on
their own as we’ll see in
later chapters.
Behavioral
cloningc
Behavioral cloning
refers to learning policies
by mimicking expert
demonstrations.
This system may struggle
with generalization to
unseen situations.
This can be incorporated
into behavior trees or into
a specific task.
Hierarchical
Task Network
(HTN)d
HTNs decompose tasks
into smaller, manageable
subtasks arranged in a
hierarchy.
These are complex to
manage and design for
very large tasks.
HTNs allow for better orga-
nization and execution of
complex tasks. This pat-
tern can be used for larger
agentic systems.
Blackboard
systemb
These systems feature
collaborative problem-solv-
ing using a shared black-
board for different
subsystems.
These systems are diffi-
cult to implement and
manage communication
between subsystems.
Agentic systems can
implement similar pat-
terns using conversation
or group chats/threads.
Genetic
algorithm (GA)d
These optimization tech-
niques are inspired by
natural selection to
evolve solutions to
solve problems.
GAs are computationally
intensive and may not
always find the optimal
solution.
GAs have potential and
could even be used to
optimize behavior trees.
a Not practical when considering complex agentic systems
b Exists in behavior trees or can easily be incorporated
c Typically applied at the task or action/condition level
d Advanced systems that would require heavy lifting when applied to agents

134
CHAPTER 6
Building autonomous assistants
In later chapters of this book, we’ll investigate some of the patterns discussed in
table 6.2. Overall, several patterns can be enhanced or incorporated using behavior
trees as the base. While other patterns, such as FSMs, may be helpful for small experi-
ments, they lack the scalability of behavior trees.
Behavior trees can provide several benefits as an AI control system, including scal-
ability. The following list highlights other notable benefits of using behavior trees:
Modularity and reusability—Behavior trees promote a modular approach to design-
ing behaviors, allowing developers to create reusable components. Nodes in a
behavior tree can be easily reused across different parts of the tree or even in
different projects, enhancing maintainability and reducing development time.
Scalability—As systems grow in complexity, behavior trees handle the addition
of new behaviors more gracefully than other methods, such as FSMs. Behavior
trees allow for the hierarchical organization of tasks, making it easier to manage
and understand large behavior sets.
Flexibility and extensibility—Behavior trees offer a flexible framework where new
nodes (actions, conditions, decorators) can be added without drastically alter-
ing the existing structure. This extensibility makes it straightforward to intro-
duce new behaviors or modify existing ones to adapt to new requirements.
Debugging and visualization—Behavior trees provide a clear and intuitive visual
representation of behaviors, which is beneficial for debugging and understand-
ing the decision-making process. Tools that support behavior trees often
include graphical editors that allow developers to visualize and debug the tree
structure, making it easier to identify and fix problems.
Decoupling of decision logic—Behavior trees separate the decision-making and
execution logic, promoting a clear distinction between high-level strategy and
low-level actions. This decoupling simplifies the design and allows for more
straightforward modifications and testing of specific behavior parts without
affecting the entire system.
Having made a strong case for behavior trees, we should now consider how to imple-
ment them in code. In the next section, we look at how to build a simple behavior
tree, using Python code.
6.1.3
Running behavior trees with Python and py_trees
Because behavior trees have been around for so long and have been incorporated into
many technologies, creating a sample demonstration is very simple. Of course, the easi-
est way is to ask ChatGPT or your favorite AI chat tool. Listing 6.1 shows the result of
using a prompt to generate the code sample and submitting figure 6.1 as the example
tree. The final code had to be corrected for simple naming and parameter errors.
NOTE
All the code for this chapter can be found by downloading the GPT
Assistants Playground project at https://mng.bz/Ea0q.

135
6.1
Introducing behavior trees
import py_trees
class HasApple(py_trees.behaviour.Behaviour):
def __init__(self, name):
super(HasApple, self).__init__(name)
def update(self):
if True:
return py_trees.common.Status.SUCCESS
else:
return py_trees.common.Status.FAILURE
# Other classes omitted…
has_apple = HasApple(name="Has apple")
eat_apple = EatApple(name="Eat apple")
sequence_1 = py_trees.composites.Sequence(name="Sequence 1", memory=True)
sequence_1.add_children([has_apple, eat_apple])
has_pear = HasPear(name="Has pear")
eat_pear = EatPear(name="Eat pear")
sequence_2 = py_trees.composites.Sequence(name="Sequence 2", memory=True)
sequence_2.add_children([has_pear, eat_pear])
root = py_trees.composites.Selector(name="Selector", memory=True)
root.add_children([sequence_1, sequence_2])
behavior_tree = py_trees.trees.BehaviourTree(root)
py_trees.logging.level = py_trees.logging.Level.DEBUG
for i in range(1, 4):
print("\n------------------ Tick {0} ------------------".format(i))
behavior_tree.tick()
### Start of output
------------------ Tick 1 ------------------
[DEBUG] Selector             : Selector.tick()
[DEBUG] Selector             : Selector.tick() [!RUNNING->reset current_child]
[DEBUG] Sequence 1           : Sequence.tick()
[DEBUG] Has apple            : HasApple.tick()
[DEBUG] Has apple            : HasApple.stop(Status.INVALID->Status.SUCCESS)
[DEBUG] Eat apple            : EatApple.tick()
Eating apple
[DEBUG] Eat apple            : EatApple.stop(Status.INVALID->Status.SUCCESS)
[DEBUG] Sequence 1           : Sequence.stop()[Status.INVALID->Status.SUCCESS]
The code in listing 6.1 represents the behavior tree in figure 6.1. You can run this
code as is or alter what the conditions return and then run the tree again. You can
also change the behavior tree by removing one of the sequence nodes from the root
selector.
Now that we have a basic understanding of behavior trees, we can move on to
working with agents/assistants. Before doing that, we’ll look at a tool to help us work
Listing 6.1
first_btree.py
Creates a class to implement
an action or condition
Creates the action
and condition nodes
Adds the nodes to
their respective
parents
Creates the action
and condition nodes
Creates the whole
behavior tree
Executes one step/tick
on the behavior tree

136
CHAPTER 6
Building autonomous assistants
with OpenAI Assistants. This tool will help us wrap our first ABTs around OpenAI
Assistants.
6.2
Exploring the GPT Assistants Playground
For the development of this book, several GitHub projects were created to address var-
ious aspects of building agents and assistants. One such project, the GPT Assistants
Playground, is built using Gradio for the interface that mimics the OpenAI Assistants
Playground but with several extras added.
The Playground project was developed as both a teaching and demonstration aid.
Inside the project, the Python code uses the OpenAI Assistants API to create a chat inter-
face and an agentic system to build and power assistants. There is also a comprehensive
collection of actions assistants you can use, and you can easily add your own actions.
6.2.1
Installing and running the Playground
The following listing shows installing and running the Playground project from the
terminal. There is currently no PyPI package to install.
# change to a working folder and create a new Python virtual environment
git clone
➥ https://github.com/cxbxmxcx/GPTAssistantsPlayground
cd GPTAssistantsPlayground
pip install -r requirements.txt
You can run the application from the terminal or using Visual Studio Code (VS
Code), with the latter giving you more control. Before running the application, you
need to set your OpenAI API key through the command line or by creating an .env
file, as we’ve done a few times already. Listing 6.3 shows an example of setting the
environment variable on Linux/Mac or the Git Bash shell (Windows recommended)
and running the application.
export OPENAI_API_KEY="your-api-key"
python main.py
Open your browser to the URL displayed (typically http://127.0.0.1:7860) or what
is mentioned in the terminal. You’ll see an interface similar to that shown in figure 6.3.
Listing 6.2
Installing the GPT Assistants Playground
Listing 6.3
Running the GPT Assistants Playground
Pulls the
source code
from GitHub
Changes directory to the
project source code folder
Installs the
requirements
Sets your API key as an
environment variable
Runs the app from the
terminal or via VS Code

137
6.2
Exploring the GPT Assistants Playground
If you’ve already defined the OpenAI Assistants, you’ll see them in the Select Assistant
dropdown.
If you’ve never defined an assistant, you can create one and choose the various
options and instructions you need. If you’ve visited the OpenAI Playground, you’ve
already experienced a similar interface.
GPT vs. an assistant
OpenAI defines a GPT as the assistant you can run and use within the ChatGPT inter-
face. An assistant can only be consumed through the API and requires custom code
in most cases. When you run an assistant, you’re charged according to the model
token usage and any special tools, including the Code Interpreter and files, whereas
a GPT runs within ChatGPT and is covered by account costs.
Select an existing Assistant
or create a new assistant.
Select from any
available models.
Select the Tools
and Actions.
The assistant
can output ﬁles
created by the
Code Interpreter.
Figure 6.3
The GPT Assistants Playground interface being used to learn math

138
CHAPTER 6
Building autonomous assistants
Each of these features is covered in more detail over the next few sections. We’ll start
with a look at using and consuming actions in the next section.
6.2.2
Using and building custom actions
Actions and tools are the building blocks that empower agents and assistants. Without
access to tools, agents are functionless chatbots. The OpenAI platform is a leader in
establishing many of the patterns for tools, as we saw in chapter 3.
The Playground provides several custom actions that can be attached to assistants
through the interface. In this next exercise, we’ll build a simple assistant and attach a
couple of custom actions to see what is possible.
Figure 6.4 shows the expanded Actions accordion, which displays many available
custom actions. Run the Playground from the terminal or debugger, and create a new
assistant. Then, select the actions shown in the figure. After you’re done selecting the
actions, scroll to the bottom, and click Add Assistant to add the assistant. Assistants
need to be created before they can be used.
After you create the assistant, you can ask it to list all available assistants. Listing the
assistants also gives you the IDs required to call the assistant. You can also call other
assistants and ask them to complete tasks in their area of specialization.
Adding your custom actions is as simple as adding code to a file and dropping it in
the right folder. Open the playground/assistant_actions folder from the main
project folder, and you’ll see several files that define the various actions. Open the
file_actions.py file in VS Code, as shown in listing 6.4.



(continued)
The reason for creating a local version of the Playground was an exercise to demon-
strate the code structure but also provide additional features listed here:
Actions (custom actions)—Creating your own actions allows you to add any
functionality you want to an assistant. As we’ll see, the Playground makes it
very easy to create your own actions quickly.
Code runner—The API does come with a Code Interpreter, but it’s relatively
expensive ($.03 per run), doesn’t allow you to install your modules, can’t run
code interactively, and runs slowly. The Playground will enable you to run
Python code locally in an isolated virtual environment. While not as secure as
pushing code out to Docker images, it does execute code windowed and out
of process better than other platforms.
Transparency and logging—The Playground provides for comprehensive cap-
turing of logs and will even show how the assistant uses internal and external
tools/actions. This can be an excellent way to see what the assistant is doing
behind the scenes.

139
6.2
Exploring the GPT Assistants Playground
import os
from playground.actions_manager import agent_action
OUTPUT_FOLDER = "assistant_outputs"
@agent_action
def save_file(filename, content):
"""
Save content to a file.
:param filename: The name of the file including extension.
:param content: The content to save in the file.
"""
file_path = os.path.join(OUTPUT_FOLDER, filename)
with open(file_path, "w", encoding="utf-8") as file:
file.write(content)
print(f"File '{filename}' saved successfully.")
Listing 6.4
playground/assistant_actions/file_actions.py
Name your assistant
a memorable name.
Ask to list the assistants, and you’ll
see all the assistants you've created.
Select call_assistant
and list_assistants.
The call_assistant action allows
the assistant to delegate work
to other assistants.
You don’t need any
special instructions.
Disable or enable the Code
Interpreter to see the effect.
Figure 6.4
Selecting and using custom actions in the interface
This decorator
automatically adds the
function as an action.
Give your functions
clear names that align
with the purpose.
The description is what
the assistant uses to
determine the function,
so document it well.
Generally returns
a message stating
success or failure

140
CHAPTER 6
Building autonomous assistants
You can add any custom action you want by placing the file in the assistant_actions
folder and decorating it with the agent_action decorator. Just make sure to give the
function a good name and enter quality documentation for how the function should
be used. When the Playground starts up, it loads all the actions in the folder that are
decorated correctly and have descriptions/documentation.
It’s that simple. You can add several custom actions as needed. In the next section,
we’ll look at a special custom action that allows the assistant to run code locally.
6.2.3
Installing the assistants database
To run several of the examples in this chapter, you’ll need to install the assistants data-
base. Fortunately, this can be easily done through the interface and just by asking
agents. The upcoming instructions detail the process for installing the assistants and
are taken directly from the GPT Assistants Playground README. You can install sev-
eral of the demo assistants located in the assistants.db SQLite database:
1
Create a new assistant, or use an existing assistant.
2
Give the assistant the create_manager_assistant action (found under the
Actions section).
3
Ask the assistant to create the manager assistant (i.e., “please create the man-
ager assistant”), and be sure to name the assistant “Manager Assistant.”
4
Refresh your browser to reload the assistants selector.
5
Select the new Manager Assistant. This assistant has the instructions and actions
that will allow it to install assistants from the assistants.db database.
6
Talk to the Manager Assistant to give you a list of assistants to install, or just ask
the Manager Assistant to install all available assistants.
6.2.4
Getting an assistant to run code locally
Getting agents and assistants to generate and run executable code has a lot of power.
Unlike the Code Interpreter, running code locally provides numerous opportunities
to iterate and tune quickly. We saw this earlier with AutoGen, where the agents could
keep running the code until it worked as expected.
In the Playground, it’s a simple matter to select the custom action run_code, as
shown in figure 6.5. You’ll also want to choose the run_shell_command action because
it allows the assistant to pip install any required modules.









141
6.2
Exploring the GPT Assistants Playground
You can now ask an assistant to generate and run the code to be sure it works on your
behalf. Try this out by adding the custom actions and asking the assistant to generate
and run code, as shown in figure 6.6. If the code doesn’t work as expected, tell the
assistant what problems you encountered.
Again, the Python code running in the Playground creates a new virtual environ-
ment in a project subfolder. This system works well if you’re not running any operat-
ing system–level code or low-level code. If you need something more robust, a good
option is AutoGen, which uses Docker containers to run isolated code.
Adding actions to run code or other tasks can make assistants feel like a black box.
Fortunately, the OpenAI Assistants API allows you to consume events and see what the
assistant is doing behind the scenes. In the next section, we’ll see what this looks like.
Do not select the
Code Interpreter tool.
Select both the run_code and
run_shell_command custom
actions. Running commands on
the shell allows an assistant to
install new packages as required.
Figure 6.5
Selecting custom actions for the assistant to run Python code

142
CHAPTER 6
Building autonomous assistants
6.2.5
Investigating the assistant process through logs
OpenAI added a feature into the Assistants API that allows you to listen to events and
actions chained through tool/action use. This feature has been integrated into the
Playground, capturing action and tool use when an assistant calls another assistant.
We can try this by asking an assistant to use a tool and then open the log. A great
example of how you can do this is by giving an assistant the Code Interpreter tool and
then asking it to plot an equation. Figure 6.7 shows an example of this exercise.
Usually, when the Assistant Code Interpreter tool is enabled, you don’t see any
code generation or execution. This feature allows you to see all tools and actions used
by the assistant as they happen. Not only is it an excellent tool for diagnostics, but it
also provides additional insights into the functions of LLMs.
We haven’t reviewed the code to do all this because it’s extensive and will likely
undergo several changes. That being said, if you plan on working with the Assistants
API, this project is a good place to start. With the Playground introduced, we can con-
tinue our journey into ABTs in the next section.
Any assistant can generate code. Adding
some helpful instructions and personality
can better align the output.
The “snake” game will open a new window
demonstrating the code is running.
Note: While the window is open, it will
block the Gradio interface.
In this example, the assistant generated
the code for the game and then realized
it need to install Pygame. After installing,
it ran the code, as shown in the side window.
Figure 6.6
Getting the assistant to generate and run Python code

143
6.3
Introducing agentic behavior trees
6.3
Introducing agentic behavior trees
Agentic behavior trees (ABTs) implement behavior trees on assistant and agent sys-
tems. The key difference between regular behavior trees and ABTs is that they use
prompts to direct actions and conditions. Because prompts may return a high occur-
rence of random results, we could also name these trees stochastic behavior trees,
which do exist. For simplicity, we’ll differentiate behavior trees used to control agents,
referring to them as agentic.
Next, we’ll undertake an exercise to create an ABT. The finished tree will be writ-
ten in Python but will require the setup and configuration of various assistants. We’ll
cover how to manage assistants using the assistants themselves.
6.3.1
Managing assistants with assistants
Fortunately, the Playground can help us quickly manage and create the assistants.
We’ll first install the Manager Assistant, followed by installing the predefined assis-
tants. let’s get started with installing the Manager Assistant using the following steps:
The Logs tab shows
where ﬁles are saved.
Code being generated and run in the Code
Interpreter is shown in the Logs tab.
Plotting a graph
is a good test
Figure 6.7
Internal assistant logs being captured

144
CHAPTER 6
Building autonomous assistants
1
Open Playground in your browser, and create a new simple assistant or use an
existing assistant. If you need a new assistant, create it and then select it.
2
With the assistant selected, open the Actions accordion, and select the create_
manager_assistant action. You don’t need to save; the interface will update
the assistant automatically.
3
Now, in the chat interface, prompt the assistant with the following: “Please cre-
ate the manager assistant.”
4
After a few seconds, the assistant will say it’s done. Refresh your browser, and
confirm that the Manager Assistant is now available. If, for some reason, the
new assistant isn’t shown, try restarting the Gradio app itself.
The Manager Assistant is like an admin that has access to everything. When engag-
ing the Manager Assistant, be sure to be specific about your requests. With the Man-
ager Assistant active, you can now install new assistants used in the book using the
following steps:
1
Select the Manager Assistant. If you’ve modified the Manager Assistant, you can
delete it and reinstall it anytime. Although it’s possible to have multiple Man-
ager Assistants, it’s not recommended.
2
Ask the Manager Assistant what assistants can be installed by typing the follow-
ing in the chat interface:
Please list all the installable assistants.
3
Identify which assistant you want installed when you ask the Manager Assistant
to install it:
Please install the Python Coding Assistant.
You can manage and install any available assistants using the Playground. You can also
ask the Manager Assistant to save the definitions of all your assistants as JSON:
Please save all the assistants as JSON to a file called assistants.json.
The Manager Assistant can access all actions, which should be considered unique and
used sparingly. When crafting assistants, it’s best to keep them goal specific and limit
the actions to just what they need. This not only avoids giving the AI too many deci-
sions but also avoids accidents or mistakes caused by hallucinations.
As we go through the remaining exercises in this chapter, you’ll likely need to
install the required assistants. Alternatively, you can ask the Manager Assistant to
install all available assistants. Either way, we look at creating an ABT with assistants in
the next section.

145
6.3
Introducing agentic behavior trees
6.3.2
Building a coding challenge ABT
Coding challenges provide a good baseline for testing and evaluating agent and assis-
tant systems. Challenges and benchmarks can quantify how well an agent or agentic
system operates. We already applied coding challenges to multi-platform agents in
